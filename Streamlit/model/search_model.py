import json
import numpy as np
import faiss
import openai
import os
import pandas as pd
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer

load_dotenv()
hf_api_key = os.getenv("HUGGINGFACE_API_KEY")
openai_client = openai.Client(api_key=os.getenv("OPENAI_API_KEY"))
"ì˜í™” ID"
# ëª¨ë¸ ë¡œë“œ
model = SentenceTransformer("BAAI/bge-m3", use_auth_token=hf_api_key)

# ì—‘ì…€ DB ë¡œë“œ
FAISS_KEY_WORD_INDEX_FILE = "data/key_word_faiss_index.idx"
KEY_WORD_INDICES_FILE = "data/key_word_indices.npy"
EXCEL_FILE_PATH = "data/Movie_DB_3262_2ì°¨.xlsx"
df = pd.read_excel(EXCEL_FILE_PATH).fillna("")

def load_faiss_indices():
    """
    FAISS ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•˜ê³ , ì—†ì„ ê²½ìš° ìƒˆë¡œ ìƒì„±
    """
    key_word_index = None
    key_word_indices = None

    if os.path.exists(FAISS_KEY_WORD_INDEX_FILE) and os.path.exists(KEY_WORD_INDICES_FILE):
        print("[INFO] ê¸°ì¡´ FAISS ì¤„ê±°ë¦¬ ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...")
        key_word_index = faiss.read_index(FAISS_KEY_WORD_INDEX_FILE)
        key_word_indices = np.load(KEY_WORD_INDICES_FILE)

    # ì¡´ìž¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒˆë¡œ ìƒì„±
    if key_word_index is None:
        print("âŒ FAISS ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤...")
        build_faiss_indices()
        return load_faiss_indices()


    return key_word_index, key_word_indices

def build_faiss_indices():
    """
    FAISS ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ê³  ì €ìž¥
    """
    # ìž„ë² ë”© ì €ìž¥ìš© ë¦¬ìŠ¤íŠ¸
    key_word_embeddings = []
    key_word_mapping = []

    # 1ï¸âƒ£ í‚¤ì›Œë“œ ìž„ë² ë”© ìƒì„± ë° ì €ìž¥
    for idx, row in df.iterrows():
        if pd.notna(row['í‚¤ì›Œë“œ']) and row['í‚¤ì›Œë“œ'].strip():  # ë¹ˆ í‚¤ì›Œë“œ ì œì™¸
            key_words = row['í‚¤ì›Œë“œ'].split(',')    # í‚¤ì›Œë“œ ë¶„í• 
            key_words = [kw.strip() for kw in key_words if kw.strip()]    # ê³µë°± ì œê±°

            for key_word in key_words:
                embedding = model.encode(key_word, convert_to_numpy=True)
                key_word_embeddings.append(embedding)
                key_word_mapping.append(idx)    # ê°ê°ì˜ í‚¤ì›Œë“œì™€ ì˜í™” ID ë§¤í•‘

    # 2ï¸âƒ£ FAISS ì¸ë±ìŠ¤ ìƒì„± ë° ì €ìž¥
    if key_word_embeddings:
        key_word_embeddings_array = np.array(key_word_embeddings)

        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ìœ„í•´ L2 ì •ê·œí™”
        faiss.normalize_L2(key_word_embeddings_array)

        # FAISS ì¸ë±ìŠ¤ ìƒì„± (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜)
        key_word_index = faiss.IndexFlatIP(key_word_embeddings_array.shape[1])  # IndexFlatIP -> Inner Product(ë‚´ì  ì§„í–‰) - (Cosine Similarity)
        key_word_index.add(key_word_embeddings_array)

        # 3ï¸âƒ£ FAISS ì¸ë±ìŠ¤ & ë§¤í•‘ëœ ì˜í™” ID ì €ìž¥
        faiss.write_index(key_word_index, FAISS_KEY_WORD_INDEX_FILE)
        np.save(KEY_WORD_INDICES_FILE, np.array(key_word_mapping))

    else:
        print("[ERROR] ì €ìž¥í•  í‚¤ì›Œë“œ ìž„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤.")



# STEP1 : ì‚¬ìš©ìž ì§ˆë¬¸ LLM ë¶„ì„ ì§„í–‰.
def analyze_question_with_llm(user_question):
    prompt = f"""
    ì‚¬ìš©ìžê°€ ì˜í™” ì¶”ì²œì„ ìš”ì²­í–ˆìŠµë‹ˆë‹¤.
    ë‹¤ìŒ ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì˜í™”ì˜ ìž¥ë¥´, ë¶„ìœ„ê¸°, í‚¤ì›Œë“œ, ê°ë…, ì£¼ì—° ë“±ì˜ ì •ë³´ë¥¼ ë¶„ì„í•˜ì„¸ìš”.
    ì§ˆë¬¸: "{user_question}"

    ì´ ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ê°€ìž¥ ì ì ˆí•œ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ì„¸ìš”.

    â€»ì£¼ìš” ê·œì¹™â€»
    1. í‚¤ì›Œë“œëŠ” í•˜ë‚˜ì˜ ëª…ì‚¬ë¡œë§Œ ìž‘ì„±í•´ì•¼í•©ë‹ˆë‹¤. (ê°ë…, ì£¼ì—° ì´ë¦„ ì œì™¸)
    - ì˜ˆì‹œ : "ë…ë¦½ìš´ë™"(O), "í•œêµ­ ì „ìŸ"(X)

    2. ë°˜ë“œì‹œ jsoní˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”.

    3. ê°ë…ì´ë‚˜ ì£¼ì—°ë°°ìš°ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ìš”ì²­ì´ ìžˆëŠ” ê²½ìš° ê°ë…/ì£¼ì—°ë°°ìš°ë¥¼ ìž…ë ¥í•œ ê°’ìœ¼ë¡œ **ë¬´ì¡°ê±´** ë°˜í™˜í•©ë‹ˆë‹¤. ì´ë•Œ, í‚¤ì›Œë“œ ê°’ì€ ë°˜í™˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    4. ë¶„ì„ ê²°ê³¼ì— ëŒ€í•œ í‚¤ì›Œë“œëŠ” 3ê°œì”© ë°˜í™˜í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ê°ë…ì´ë‚˜ ì£¼ì—°ë°°ìš°ë¥¼ ì°¾ëŠ” ì§ˆë¬¸ì—ì„œëŠ” í‚¤ì›Œë“œë¥¼ ë°˜í™˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    - ì˜ˆì‹œ : {{ "í‚¤ì›Œë“œ": ["í¬ë¦¬ìŠ¤ë§ˆìŠ¤", "ê²¨ìš¸", "ëˆˆ", "ì—°ë§"]}}
    - ì˜ˆì‹œ : "oooê°€ ë‚˜ì˜¤ëŠ”", "oooì´ ë‚˜ì˜¤ëŠ”", "oooì´ ì¶œì—°í•œ" ë“±ì˜ ì§ˆë¬¸ì€ ê°ë…ì´ë‚˜ ì£¼ì—°ë°°ìš°ë¥¼ ì°¾ëŠ” ì§ˆë¬¸ì´ê¸° ë•Œë¬¸ì— í‚¤ì›Œë“œ ê°’ì„ ë°˜í™˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

    5. ì§ˆë¬¸ì˜ í•µì‹¬ì ì¸ ìš”ì†Œ í•˜ë‚˜ë¥¼ í‚¤ì›Œë“œì— í¬í•¨ì‹œí‚µë‹ˆë‹¤.

    6. ì§ˆë¬¸ì— í•„ìš”í•˜ì§€ ì•ŠëŠ” ê°’ì€ ì ˆëŒ€ë¡œ ë°˜í™˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    - ì˜ˆì‹œ : "ë´‰ì¤€í˜¸ ê°ë…ì˜ ì˜í™” ì¶”ì²œí•´ì¤˜"ì™€ ê°™ì€ ì§ˆë¬¸ì˜ ê²½ìš°, "í‚¤ì›Œë“œ", "ì£¼ì—°" ìš”ì†Œê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    - ì˜ˆì‹œ : "í†° í¬ë£¨ì¦ˆê°€ ë‚˜ì˜¤ëŠ” ì˜í™” ì¶”ì²œí•´ì¤˜"ì™€ ê°™ì€ ì§ˆë¬¸ì˜ ê²½ìš°, "í‚¤ì›Œë“œ", "ê°ë…" ìš”ì†Œê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    - ì˜ˆì‹œ : "ê²¨ìš¸ì— ë³¼ë§Œí•œ ì˜í™” ì¶”ì²œí•´ì¤˜"ì™€ ê°™ì€ ì§ˆë¬¸ì˜ ê²½ìš°, "ê°ë…", "ì£¼ì—°"ì˜ ìš”ì†Œê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    
    7. ì‹œë¦¬ì¦ˆ ì˜í™” ì¶”ì²œì„ ìš”êµ¬ë°›ì€ ê²½ìš°ì—” ì£¼ì—°ì´ë‚˜ ê°ë…ëª…ì„ ì‚¬ìš©í•´ë„ ë©ë‹ˆë‹¤. ë‹¤ë§Œ ì‹œë¦¬ì¦ˆë§ˆë‹¤ ì˜í™” ê°ë…ê³¼ ë°°ìš°ê°€ ë³€ê²½ë˜ëŠ” ê²½ìš°ê°€ ìžˆê¸° ë•Œë¬¸ì— ì´ë¥¼ ê³ ë ¤í•´ì•¼í•©ë‹ˆë‹¤.

    """

    response = openai_client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "system", "content": "ë‹¹ì‹ ì€ ì˜í™” ì¶”ì²œ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤."},
                  {"role": "user", "content": prompt}],
    )
    # ðŸ”¹ ì‘ë‹µ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
    raw_content = response.choices[0].message.content
    print("[INFO] GPT ì‘ë‹µ:", raw_content)  # ë””ë²„ê¹…ìš© ì¶œë ¥

    # ë¹ˆ ì‘ë‹µ ì²´í¬
    if not raw_content:
        print("âŒ GPT ì‘ë‹µì´ ë¹„ì–´ ìžˆìŠµë‹ˆë‹¤.")
        return {}

    # JSON ì½”ë“œ ë¸”ë¡ ì œê±° (`json.loads()`ë¥¼ ì ìš©í•˜ê¸° ì „ ì •ë¦¬)
    if raw_content.startswith("```json"):
        raw_content = raw_content.strip("```json").strip("```").strip()

    try:
        parsed_json = json.loads(raw_content)  # ðŸ”¹ JSON ë³€í™˜
        return parsed_json
    except json.JSONDecodeError as e:
        print(f"âŒ JSON ë³€í™˜ ì‹¤íŒ¨: {e}")
        return {}
    


def search_movies_with_faiss(expanded_keywords, top_k=300):
    """
    - ê°ë…ì´ë‚˜ ì£¼ì—° ë°°ìš°ê°€ ìžˆëŠ” ê²½ìš°: ì „ì²´ DBì—ì„œ ì§ì ‘ í•„í„°ë§ (FAISS ì‚¬ìš© X)
    - í•µì‹¬ í‚¤ì›Œë“œê°€ ìžˆëŠ” ê²½ìš°: í•´ë‹¹ í‚¤ì›Œë“œë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ê²€ìƒ‰ í›„ ê¸°ì¡´ ê²€ìƒ‰ ê²°ê³¼ì™€ ë³‘í•©
    """
    key_word_index, key_word_indices = load_faiss_indices()

    print('[DEBUG] í‚¤ì›Œë“œ í™•ì¸ : ', expanded_keywords)
    # ì˜í™” ID â†’ ì˜í™” ì´ë¦„ ë³€í™˜ì„ ìœ„í•œ ë”•ì…”ë„ˆë¦¬ ìƒì„±
    expanded_keywords.setdefault("í‚¤ì›Œë“œ", [])
    expanded_keywords.setdefault("ê°ë…", [])
    expanded_keywords.setdefault("ì£¼ì—°", [])

    # ê°ë…, ì£¼ì—°ì´ ë¬¸ìžì—´ì¸ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    if isinstance(expanded_keywords["ê°ë…"], str):
        expanded_keywords["ê°ë…"] = [expanded_keywords["ê°ë…"]]
    if isinstance(expanded_keywords["ì£¼ì—°"], str):
        expanded_keywords["ì£¼ì—°"] = [expanded_keywords["ì£¼ì—°"]]

    result_df = pd.DataFrame()

    # ðŸ”¹ 1. ê°ë…/ë°°ìš° ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜ í•„í„°ë§ -> ì •ë‹µì´ ì •í•´ì ¸ ìžˆê¸° ë•Œë¬¸ì— DFì—ì„œ ì§ì ‘ ë°˜í™˜
    
    if expanded_keywords["ê°ë…"] or expanded_keywords["ì£¼ì—°"]:
        def has_common_element(db_list, query_list):
            if not db_list or not query_list:
                return False
            db_set = set(db_list.split(", "))
            query_set = set(query_list)
            return not db_set.isdisjoint(query_set)

        director_filter = df["ê°ë…"].apply(lambda x: has_common_element(x, expanded_keywords["ê°ë…"]))
        actor_filter = df["ì£¼ì—°"].apply(lambda x: has_common_element(x, expanded_keywords["ì£¼ì—°"]))

        result_df = df[director_filter | actor_filter]
        print(result_df)

        if not result_df.empty:
            return result_df

    # ðŸ”¹ 2. ê²€ìƒ‰ìš© í‚¤ì›Œë“œ ìž„ë² ë”© ìƒì„±
    search_key_word_embeddings = []

    for keyword in expanded_keywords['í‚¤ì›Œë“œ']:
        if isinstance(keyword, str) and keyword.strip():  # ë¹ˆ í‚¤ì›Œë“œ ì œì™¸
            embedding = model.encode(keyword.strip(), convert_to_numpy=True)
            search_key_word_embeddings.append(embedding)

    # NumPy ë°°ì—´ë¡œ ë³€í™˜
    if search_key_word_embeddings:
        search_key_word_embeddings_array = np.array(search_key_word_embeddings)
        faiss.normalize_L2(search_key_word_embeddings_array)
    else:
        return pd.DataFrame()  # í‚¤ì›Œë“œê°€ ì—†ì„ ê²½ìš° ë¹ˆ ë°ì´í„°í”„ë ˆìž„ ë°˜í™˜

    # ðŸ”¹ 3. í‚¤ì›Œë“œ ê¸°ë°˜ í•„í„°ë§ : 1~5ë²ˆ ê° í‚¤ì›Œë“œ ìœ ì‚¬ ì˜í™” ë°˜í™˜ -> êµì§‘í•© êµ¬í•˜ê¸°
    _, search_indices_1 = key_word_index.search(search_key_word_embeddings_array[0].reshape(1, -1), top_k)
    search_results_1 = set(int(key_word_indices[n]) for n in search_indices_1[0])    # key_word_indices[n] -> np.int êµì§‘í•©ì´ ì•ˆêµ¬í•´ì§ -> intë¡œ ë³€ê²½

    _, search_indices_2 = key_word_index.search(search_key_word_embeddings_array[1].reshape(1, -1), top_k)
    search_results_2 = set(int(key_word_indices[n]) for n in search_indices_2[0])

    _, search_indices_3 = key_word_index.search(search_key_word_embeddings_array[2].reshape(1, -1), top_k)
    search_results_3 = set(int(key_word_indices[n]) for n in search_indices_3[0])
    
    # ðŸ”¹ 4. ë‹¤ì„¯ ê°œì˜ ê²°ê³¼ì—ì„œ êµì§‘í•©ì„ êµ¬í•¨
    final_recommended_movies = search_results_1 & search_results_2 & search_results_3

    # 3ï¸âƒ£ ë¦¬ìŠ¤íŠ¸ ë³€í™˜ í›„ ê²°ê³¼ ì¶œë ¥
    recommended_movie_ids = list(final_recommended_movies)

    print("ðŸ” ì¶”ì²œ ì˜í™” ID ëª©ë¡:", recommended_movie_ids)

    # 5ï¸âƒ£ ì˜í™” IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ DataFrame í•„í„°ë§
    recommended_movies_df = df.loc[df.index.isin(recommended_movie_ids)]

    return recommended_movies_df  # âœ… ê²°ê³¼ ë°˜í™˜


# STEP4: LLMì„ í™œìš©í•œ ìµœì¢… ì¶”ì²œ ìƒì„±
def generate_recommendations(user_question, search_results, max_results=8, batch_size=10):
    if search_results.empty:
        return []

    movie_data = search_results[['ì˜í™” ì œëª©', 'ê°œë´‰ì¼', 'ìž¥ë¥´', 'ê°ë…', 'ì£¼ì—°', 'ì¤„ê±°ë¦¬', 'í‚¤ì›Œë“œ']].to_dict(orient='records')
    total_movies = len(movie_data)

    # LLM í˜¸ì¶œì„ Batch ë‹¨ìœ„ë¡œ ìˆ˜í–‰í•˜ì—¬ RateLimit ë°©ì§€
    recommended_movies = []
    for i in range(0, total_movies, batch_size):
        batch = movie_data[i:i+batch_size]

        prompt = f"""
        ë‹¹ì‹ ì€ ì˜í™”ë¥¼ ì¶”ì²œí•˜ëŠ” ì˜í™” ì¶”ì²œ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤.
        ì‚¬ìš©ìžê°€ ì˜í™” ì¶”ì²œì„ ìš”ì²­í–ˆìŠµë‹ˆë‹¤.
        
        ì§ˆë¬¸: "{user_question}"
        ì‚¬ìš©ìžê°€ ì›í•˜ëŠ” ì¶”ì²œ ê°œìˆ˜: ìµœëŒ€ {max_results}ê°œ
        ì œê³µëœ ì˜í™” ë°ì´í„°: {json.dumps(batch, ensure_ascii=False)}
        
        ì§ˆë¬¸ì„ ì •í™•í•˜ê²Œ ì´í•´í•˜ê³ , ì‚¬ìš©ìžì—ê²Œ ì•Œë§žëŠ” ì˜í™”ë¥¼ ì¶”ì²œí•´ì•¼í•©ë‹ˆë‹¤.
        ì§ˆë¬¸ì„ ë°˜ë“œì‹œ ìˆ™ì§€í•˜ì„¸ìš”.

        â€» ì¤‘ìš”í•œ ê·œì¹™ â€»
        1. ì œê³µëœ 'ì˜í™” ë°ì´í„°' ë‚´ì—ì„œë§Œ ì¶”ì²œí•˜ì„¸ìš”. (ìƒˆë¡œìš´ ì˜í™”ë¥¼ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.)
        2. ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ì—†ëŠ” ì˜í™”ëŠ” ì ˆëŒ€ ì¶”ì²œí•˜ì§€ ë§ˆì„¸ìš”.
        ë‹µë³€ì„ ë°˜í™˜í•˜ê¸° ì „, ë‹¤ì‹œ í•œë²ˆ ì´ ì˜í™”ê°€ ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ì–´ìš¸ë¦¬ëŠ” ì˜í™”ì¸ì§€, DBì— ìžˆëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”.

        JSON í˜•ì‹ìœ¼ë¡œ ì˜í™” ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ì„¸ìš”. JSON ì´ì™¸ì˜ í…ìŠ¤íŠ¸ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
        {json.dumps(batch, ensure_ascii=False)}

        ì˜ˆì‹œ:
        {{"ì¶”ì²œ ì˜í™”": ["ì˜í™”1", "ì˜í™”2", "ì˜í™”3"]}}
        """

        response = openai_client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "system", "content": "ë‹¹ì‹ ì€ ì˜í™” ì¶”ì²œ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}],
        )

        try:
            batch_recommendations = json.loads(response.choices[0].message.content).get("ì¶”ì²œ ì˜í™”", [])
            # "ì¶”ì²œ ì˜í™”"ê°€ dict í˜•íƒœë¡œ ë“¤ì–´ì˜¤ì§€ ì•Šë„ë¡ í•„í„°ë§ (ì œëª©ë§Œ ì¶”ì¶œ)
            batch_recommendations = [
                movie["ì˜í™” ì œëª©"] if isinstance(movie, dict) else movie
                for movie in batch_recommendations
            ]
            recommended_movies.extend(batch_recommendations)
        except json.JSONDecodeError:
            print("âŒ LLMì—ì„œ ìž˜ëª»ëœ ì‘ë‹µì„ ë°›ì•˜ìŠµë‹ˆë‹¤.")

                # âœ… ìµœëŒ€ ê²°ê³¼ ê°œìˆ˜ë§Œí¼ë§Œ ìœ ì§€
        if len(recommended_movies) >= max_results:
            break

    return recommended_movies[:max_results]

