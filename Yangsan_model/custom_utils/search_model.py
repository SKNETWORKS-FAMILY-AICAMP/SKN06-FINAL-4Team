import pandas as pd
import faiss
import numpy as np
import json
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from dotenv import load_dotenv
import openai
import os

load_dotenv()
hf_api_key = os.getenv("HUGGINGFACE_API_KEY")
openai_client = openai.Client(api_key=os.getenv("OPENAI_API_KEY"))

# ìž„ë² ë”© ëª¨ë¸ ë¡œë“œ
model = SentenceTransformer("all-MiniLM-L6-v2", use_auth_token=hf_api_key)

# ì—‘ì…€ DB ë¡œë“œ
EXCEL_FILE_PATH = "../../data/MOVIE_DB/MOVIE_DB_7005.xlsx"
df = pd.read_excel(EXCEL_FILE_PATH).fillna("")
review_db_path = "../../data/MOVIE_DB/Review_in_DB.xlsx"
review_df = pd.read_excel(review_db_path)

# FAISS ì¸ë±ìŠ¤ ì €ìž¥ íŒŒì¼ ê²½ë¡œ
FAISS_INDEX_FILE = "faiss_index.bin"
MOVIE_INDICES_FILE = "movie_indices.npy"

######################################################################
# í‰ê·  ìž„ë² ë”©
    # ì—¬ëŸ¬ ê°œì˜ ë²¡í„°ë¥¼ í‰ê·  ë‚´ì–´ í•˜ë‚˜ì˜ ë²¡í„°í™” ì§„í–‰ -> ë²¡í„°ì˜ ê°œìˆ˜ê°€ ë‹¤ë¥´ë”ë¼ë„ í•­ìƒ ê°™ì€ ì°¨ì›ì˜ ë²¡í„°ë¥¼ ìœ ì§€í•  ìˆ˜ ìžˆë„ë¡
    # ë²¡í„°ì˜ í¬ê¸°(norm)ëŠ” ì¼ì •í•˜ì§€ ì•Šì„ ìˆ˜ ìžˆìŒ
# ì •ê·œí™”
    # ë²¡í„° í¬ê¸°ë¥¼ 1ë¡œ ë§žì¶¤ -> ë²¡í„° ê°„ì˜ ìœ ì‚¬ë„ ë¹„êµê°€ ê³µì •í•´ì§ˆ ìˆ˜ ìžˆë„ë¡
    # FAISSëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë‚´ì ì„ í†µí•´ ìœ ì‚¬ë„ ê³„ì‚°í•¨. -> ì •ê·œí™” í•˜ë©´ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì™€ ë™ì¼í•œ íš¨ê³¼ë¥¼ ê°€ì§
#####################################################################

def build_faiss_index():
    """
    ì´ í•¨ìˆ˜ëŠ” í‚¤ì›Œë“œ ìˆ˜ì— ë”°ë¥¸ ì™¸ê³¡ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì•„ëž˜ì™€ ê°™ì€ ë°©ë²•ìœ¼ë¡œ ìž„ë² ë”© ë²¡í„°í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.

    - ê° ì˜í™”ì— ëŒ€í•´ **ë¬¸ìž¥**ì„ ìƒì„±í•˜ì—¬ ìž„ë² ë”©
    - ê° ìž„ë² ë”©(ìž¥ë¥´, ì†Œê°œ, í‚¤ì›Œë“œ)ì„ í‰ê·  ë‚´ì–´ ë‹¨ì¼ ë²¡í„°ë¡œ ìƒì„±
    - L2 ì •ê·œí™” (ë°±í„° í¬ê¸° ê³ ì •)
    - FAISSì— ì¸ë±ìŠ¤ ì¶”ê°€
    """
    print("[INFO] ë‹¤ì¤‘ ë²¡í„° ê¸°ë°˜ FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")

    # ìž„ë² ë”© ì°¨ì› í™•ì¸
    sample_text = "ìƒ˜í”Œ í…ìŠ¤íŠ¸"
    d = model.encode([sample_text], convert_to_numpy=True).shape[1]

    # ðŸ”¹ ì €ìž¥í•  ìž„ë² ë”© ë¦¬ìŠ¤íŠ¸ ë° ì¸ë±ìŠ¤ ë§¤í•‘ ë¦¬ìŠ¤íŠ¸
    all_embeddings = []
    movie_mapping = []

    # ðŸ”¹ ê° ì˜í™”ì— ëŒ€í•œ ìž„ë² ë”© ìˆ˜í–‰
    for idx, row in df.iterrows():
        embeddings_list = []

        # ðŸ”¹ ìž¥ë¥´ ë¬¸ìž¥ ë³€í™˜ ë° ìž„ë² ë”©
        if pd.notna(row['ìž¥ë¥´']):
            genre_sentence = f"ì´ ì˜í™”ì˜ ìž¥ë¥´ëŠ” {row['ìž¥ë¥´']} ìž…ë‹ˆë‹¤."
            genre_embedding = model.encode([genre_sentence], convert_to_numpy=True)[0]
            embeddings_list.append(genre_embedding)
        else:
            embeddings_list.append(np.zeros(d))
            
        # ðŸ”¹ í‚¤ì›Œë“œ ë¬¸ìž¥ ë³€í™˜ ë° ìž„ë² ë”©
        if pd.notna(row['í‚¤ì›Œë“œ(í•œê¸€)']):
            keyword_sentence = f"ì´ ì˜í™”ì˜ ì£¼ìš” í‚¤ì›Œë“œëŠ” {row['í‚¤ì›Œë“œ(í•œê¸€)']} ìž…ë‹ˆë‹¤."
            keyword_embedding = model.encode([keyword_sentence], convert_to_numpy=True)[0]
            embeddings_list.append(keyword_embedding)
        else:
            embeddings_list.append(np.zeros(d))

        # ðŸ”¹ ì‹œë†‰ì‹œìŠ¤ ë¬¸ìž¥ ë³€í™˜ ë° ìž„ë² ë”©
        if pd.notna(row['ì†Œê°œ']):
            synopsis_sentence = f"ì´ ì˜í™”ì˜ ì¤„ê±°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: {row['ì†Œê°œ']}"
            synopsis_embedding = model.encode([synopsis_sentence], convert_to_numpy=True)[0]
            embeddings_list.append(synopsis_embedding)
        else:
            embeddings_list.append(np.zeros(d))

        # ðŸ”¹ í‰ê·  ìž„ë² ë”© + ì •ê·œí™”
        movie_embedding = np.mean(embeddings_list, axis=0)  # í‰ê·  ìž„ë² ë”© ê³„ì‚°
        movie_embedding /= np.linalg.norm(movie_embedding)  # L2 ì •ê·œí™” - ë²¡í„° í¬ê¸° 1ë¡œ ê³ ì •

        all_embeddings.append(movie_embedding)
        movie_mapping.append(idx)  # í•´ë‹¹ ì˜í™”ì˜ ID ì €ìž¥

    # ðŸ”¹ ë²¡í„° ë°°ì—´ ë³€í™˜
    all_embeddings = np.vstack(all_embeddings)

    # ðŸ”¹ FAISS ì¸ë±ìŠ¤ ìƒì„± ë° ì €ìž¥
    index = faiss.IndexFlatIP(d)  # ë‚´ì  ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰
    index.add(all_embeddings)

    faiss.write_index(index, FAISS_INDEX_FILE)
    np.save("movie_indices.npy", np.array(movie_mapping))

    print("[INFO] FAISS ì¸ë±ìŠ¤ ì €ìž¥ ì™„ë£Œ.")


# FAISS ì¸ë±ìŠ¤ ë¡œë“œ (ì—†ìœ¼ë©´ ìƒì„±)
def load_faiss_index():
    if os.path.exists(FAISS_INDEX_FILE) and os.path.exists(MOVIE_INDICES_FILE):
        print("[INFO] ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...")
        index = faiss.read_index(FAISS_INDEX_FILE)
        movie_indices = np.load(MOVIE_INDICES_FILE)
        return index, movie_indices
    else:
        print("âŒ FAISS ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤...")
        build_faiss_index()  # FAISS ì¸ë±ìŠ¤ ìƒì„±
        index = faiss.read_index(FAISS_INDEX_FILE)
        movie_indices = np.load(MOVIE_INDICES_FILE)
        return index, movie_indices
    

# ì œì™¸í•  í‚¤ì›Œë“œ í•„í„°ë§í•˜ëŠ” í•¨ìˆ˜
def filter_movies_by_exclusion(df, exclusion_keywords, threshold=0.8):
    if not exclusion_keywords:
        return df  # ì œì™¸í•  í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜

    exclusion_embeddings = model.encode(exclusion_keywords, convert_to_numpy=True)  # ðŸ”¹ ì œì™¸í•  í‚¤ì›Œë“œ ë²¡í„°í™”

    def is_relevant(row_keywords):
        if not row_keywords:
            return True  # í‚¤ì›Œë“œê°€ ì—†ëŠ” ê²½ìš° í•„í„°ë§ ëŒ€ìƒ ì•„ë‹˜

        movie_keywords = row_keywords.split(", ")  # ðŸ”¹ ì˜í™” í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸í™”
        movie_embeddings = model.encode(movie_keywords, convert_to_numpy=True)  # ðŸ”¹ ì˜í™” í‚¤ì›Œë“œ ë²¡í„°í™”

        # ðŸ”¹ ì œì™¸í•  í‚¤ì›Œë“œì™€ ì˜í™” í‚¤ì›Œë“œ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = cosine_similarity(movie_embeddings, exclusion_embeddings)

        # ðŸ”¹ ìœ ì‚¬ë„ê°€ threshold ì´ìƒì¸ í‚¤ì›Œë“œê°€ ìžˆìœ¼ë©´ ì œì™¸
        return not (similarities >= threshold).any()

    mask = df["í‚¤ì›Œë“œ(í•œê¸€)"].apply(is_relevant)  # ðŸ”¹ í•´ë‹¹ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì˜í™”ë§Œ ë‚¨ê¹€
    return df[mask]



# STEP1 : ì‚¬ìš©ìž ì§ˆë¬¸ LLM ë¶„ì„ ì§„í–‰.
def analyze_question_with_llm(user_question):
    prompt = f"""
    ì‚¬ìš©ìžê°€ ì˜í™” ì¶”ì²œì„ ìš”ì²­í–ˆìŠµë‹ˆë‹¤.
    ë‹¤ìŒ ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì˜í™”ì˜ ìž¥ë¥´, ë¶„ìœ„ê¸°, í‚¤ì›Œë“œ, ê°ë…, ì£¼ì—° ë“±ì˜ ì •ë³´ë¥¼ ë¶„ì„í•˜ì„¸ìš”.
    ì§ˆë¬¸: "{user_question}"

    ì´ ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ê°€ìž¥ ì ì ˆí•œ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ì„¸ìš”.

    "í•µì‹¬ í‚¤ì›Œë“œ"ëŠ” ì§ˆë¬¸ì—ì„œ ê°€ìž¥ ì¤‘ìš”í•œ ìš”ì†Œì´ë©°, íŠ¹ì •í•œ ë°°ê²½, í…Œë§ˆ, ê³„ì ˆ, ê°ì„± ë“±ì„ ë‚˜íƒ€ë‚¼ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
    "í•µì‹¬ í‚¤ì›Œë“œ"ê°€ ì—†ëŠ” ê²½ìš°ì—ëŠ” ë¹ˆ ê°’ìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”.

    - ì˜ˆì œ:
        - "í¬ë¦¬ìŠ¤ë§ˆìŠ¤ ì˜í™” ì¶”ì²œí•´ì¤˜" â†’ í•µì‹¬ í‚¤ì›Œë“œ: ["í¬ë¦¬ìŠ¤ë§ˆìŠ¤"]
        - "ë¹„ ì˜¤ëŠ” ë‚  ë³¼ ì˜í™” ì¶”ì²œí•´ì¤˜" â†’ í•µì‹¬ í‚¤ì›Œë“œ: ["ë¹„"]
        - "ì—°ì¸ì´ëž‘ ë³¼ë§Œí•œ ì˜í™” ì¶”ì²œí•´ì¤˜" â†’ í•µì‹¬ í‚¤ì›Œë“œ: ["ë¡œë§¨ìŠ¤"]
        - "ìž¬ë¯¸ìžˆëŠ” ì˜í™” ì¶”ì²œí•´ì¤˜" â†’ í•µì‹¬ í‚¤ì›Œë“œ: []
        - "ê°€ì¡±ê³¼ ë³¼ ë§Œí•œ ì˜í™” ì¶”ì²œí•´ì¤˜" â†’ í•µì‹¬ í‚¤ì›Œë“œ: []
    
    "í•µì‹¬ í‚¤ì›Œë“œ"ëŠ” 1ê°œë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.

    JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”.
    ì˜ˆì‹œ: {{"ìž¥ë¥´": ["ë¡œë§¨ìŠ¤", "ì½”ë¯¸ë””", "ë“œë¼ë§ˆ"], "ë¶„ìœ„ê¸°": ["ê°ì„±ì ì¸", "ë”°ëœ»í•œ"], "í‚¤ì›Œë“œ": ["í¬ë¦¬ìŠ¤ë§ˆìŠ¤", "ê²¨ìš¸", "ëˆˆ", "ì—°ë§"], "ê°ë…": "ë´‰ì¤€í˜¸", "ì£¼ì—°": "ì†¡ê°•í˜¸", "ì œì™¸í•  í‚¤ì›Œë“œ": ["í­ë ¥", "ê°•ê°„", "ìž”ì¸í•œ", "í­í–‰", "ìš°ìš¸", "ì‚´ì¸"], "í•µì‹¬ í‚¤ì›Œë“œ" : ["í¬ë¦¬ìŠ¤ë§ˆìŠ¤"]}}
    
    ë¶„ì„ ê²°ê³¼ì— ëŒ€í•œ ìž¥ë¥´ 3ê°œ, ë¶„ìœ„ê¸° 3ê°œ, í‚¤ì›Œë“œì™€ ì œì™¸í•  í‚¤ì›Œë“œëŠ” ê°ê° 5ê°œì”© ë°˜í™˜í•©ë‹ˆë‹¤.
    
    ì§ˆë¬¸ì„ ë¶„ì„í•œ ë’¤ì— í•„ìš”ì¹˜ ì•ŠëŠ” KEYê°’ì€ ë°˜í™˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    ì˜ˆì‹œ: "ë´‰ì¤€í˜¸ ê°ë…ì˜ ì˜í™” ì¶”ì²œí•´ì¤˜"ì™€ ê°™ì€ ì§ˆë¬¸ì˜ ê²½ìš°, "ìž¥ë¥´", "ë¶„ìœ„ê¸°", "í‚¤ì›Œë“œ", "ì£¼ì—°", "ì œì™¸í•  í‚¤ì›Œë“œ" ë“±ì˜ ìš”ì†Œê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë°˜ë©´ì— "ê²¨ìš¸ì— ë³¼ë§Œí•œ ì˜í™” ì¶”ì²œí•´ì¤˜"ì™€ ê°™ì€ ì§ˆë¬¸ì˜ ê²½ìš°, "ê°ë…", "ì£¼ì—°" ë“±ì˜ ìš”ì†Œê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    íŠ¹ì • ì‹œë¦¬ì¦ˆ ì˜í™” ì¶”ì²œì„ ì›í•  ê²½ìš°, ì‹œë¦¬ì¦ˆë§ˆë‹¤ ì˜í™” ê°ë…ê³¼ ë°°ìš°ê°€ ë³€ê²½ë˜ëŠ” ê²½ìš°ê°€ ìžˆê¸° ë•Œë¬¸ì— ì´ë¥¼ ê³ ë ¤í•´ì•¼í•©ë‹ˆë‹¤.
    """

    response = openai_client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "system", "content": "ë‹¹ì‹ ì€ ì˜í™” ì¶”ì²œ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤."},
                  {"role": "user", "content": prompt}],
    )
    # ðŸ”¹ ì‘ë‹µ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
    raw_content = response.choices[0].message.content
    print("[INFO] GPT ì‘ë‹µ:", raw_content)  # ë””ë²„ê¹…ìš© ì¶œë ¥

    # ë¹ˆ ì‘ë‹µ ì²´í¬
    if not raw_content:
        print("âŒ GPT ì‘ë‹µì´ ë¹„ì–´ ìžˆìŠµë‹ˆë‹¤.")
        return {}

    # JSON ì½”ë“œ ë¸”ë¡ ì œê±° (`json.loads()`ë¥¼ ì ìš©í•˜ê¸° ì „ ì •ë¦¬)
    if raw_content.startswith("```json"):
        raw_content = raw_content.strip("```json").strip("```").strip()

    try:
        parsed_json = json.loads(raw_content)  # ðŸ”¹ JSON ë³€í™˜
        return parsed_json
    except json.JSONDecodeError as e:
        print(f"âŒ JSON ë³€í™˜ ì‹¤íŒ¨: {e}")
        return {}


# STEP2: 1ì°¨ í•„í„°ë§
def search_movies_with_keywords(expanded_keywords, top_k=100):  # ì²˜ìŒ top_k = 100 
    """
    - ê°ë…ì´ë‚˜ ì£¼ì—° ë°°ìš°ê°€ ìžˆëŠ” ê²½ìš°: ì „ì²´ DBì—ì„œ ì§ì ‘ í•„í„°ë§ (FAISS ì‚¬ìš© X)
    - í•µì‹¬ í‚¤ì›Œë“œê°€ ìžˆëŠ” ê²½ìš°: í•´ë‹¹ í‚¤ì›Œë“œë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ê²€ìƒ‰ í›„ ê¸°ì¡´ ê²€ìƒ‰ ê²°ê³¼ì™€ ë³‘í•©
    """
    index, movie_indices = load_faiss_index()

    if index is None or movie_indices is None:
        return pd.DataFrame()  # ì¸ë±ìŠ¤ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë°ì´í„°í”„ë ˆìž„ ë°˜í™˜

    expanded_keywords.setdefault("ìž¥ë¥´", [])
    expanded_keywords.setdefault("ë¶„ìœ„ê¸°", [])
    expanded_keywords.setdefault("í‚¤ì›Œë“œ", [])
    expanded_keywords.setdefault("í•µì‹¬ í‚¤ì›Œë“œ", [])  # ðŸ”¹ ë¦¬ìŠ¤íŠ¸ë¡œ ì„¤ì •
    expanded_keywords.setdefault("ì œì™¸í•  í‚¤ì›Œë“œ", [])
    expanded_keywords.setdefault("ê°ë…", [])
    expanded_keywords.setdefault("ì£¼ì—°", [])

    result_df = pd.DataFrame()

    # ðŸ”¹ 1. ê°ë…/ë°°ìš° ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜ í•„í„°ë§
    if expanded_keywords["ê°ë…"] or expanded_keywords["ì£¼ì—°"]:
        def has_common_element(db_list, query_list):
            if not db_list or not query_list:
                return False
            db_set = set(db_list.split(", "))
            query_set = set(query_list)
            return not db_set.isdisjoint(query_set)

        director_filter = df["ê°ë…"].apply(lambda x: has_common_element(x, expanded_keywords["ê°ë…"]))
        actor_filter = df["ì£¼ì—°"].apply(lambda x: has_common_element(x, expanded_keywords["ì£¼ì—°"]))

        result_df = df[director_filter | actor_filter]
        result_df = filter_movies_by_exclusion(result_df, expanded_keywords["ì œì™¸í•  í‚¤ì›Œë“œ"])

        if not result_df.empty:
            return result_df

    # ðŸ”¹ 2. í•µì‹¬ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ (ë¬¸ìžì—´ ê¸°ë°˜ í•„í„°ë§)
    core_keyword_results = pd.DataFrame()

    if expanded_keywords["í•µì‹¬ í‚¤ì›Œë“œ"]:
        core_keyword = expanded_keywords["í•µì‹¬ í‚¤ì›Œë“œ"][0] if expanded_keywords["í•µì‹¬ í‚¤ì›Œë“œ"] else ""

        # ðŸ”¹ FAISS ê²€ìƒ‰ ìˆ˜í–‰
        core_query_embedding = model.encode([core_keyword], convert_to_numpy=True)
        core_query_embedding /= (np.linalg.norm(core_query_embedding) + 1e-10)  # ì •ê·œí™”

        core_query_embedding = core_query_embedding.reshape(1, -1)  # FAISSëŠ” 2D ë°°ì—´ í•„ìš”
        _, core_indices = index.search(core_query_embedding, top_k // 2)  # í•µì‹¬ í‚¤ì›Œë“œ ì¤‘ì‹¬ ê²€ìƒ‰

        # ðŸ”¹ ê²€ìƒ‰ëœ ì˜í™” ì¸ë±ìŠ¤ ë³€í™˜ ë° ì¤‘ë³µ ì œê±°
        core_movie_indices = [movie_indices[i] for i in core_indices[0]]
        core_keyword_results = df.iloc[core_movie_indices].drop_duplicates(subset=["ì˜í™” ì œëª©"])
        core_keyword_results = filter_movies_by_exclusion(core_keyword_results, expanded_keywords["ì œì™¸í•  í‚¤ì›Œë“œ"])

    # ðŸ”¹ 3. FAISS ê²€ìƒ‰ (í‚¤ì›Œë“œ + ë¶„ìœ„ê¸° & ìž¥ë¥´ ê°œë³„ ìž„ë² ë”©)
    keyword_mood_text = ", ".join(expanded_keywords["í‚¤ì›Œë“œ"] + expanded_keywords["ë¶„ìœ„ê¸°"])
    genre_text = ", ".join(expanded_keywords["ìž¥ë¥´"])
    
    if not keyword_mood_text:
        keyword_mood_text = "ì˜í™”"
    if not genre_text:
        genre_text = "ì˜í™” ìž¥ë¥´"

    keyword_mood_embedding = model.encode([keyword_mood_text], convert_to_numpy=True)
    genre_embedding = model.encode([genre_text], convert_to_numpy=True)

    # ðŸ”¹ ë‘ ë²¡í„°ë¥¼ í‰ê·  ë‚´ì–´ ìµœì¢… ê²€ìƒ‰ ë²¡í„° ìƒì„±
    query_embedding = (keyword_mood_embedding + genre_embedding) / 2
    query_embedding /= np.linalg.norm(query_embedding)  # ì •ê·œí™”

    query_embedding = query_embedding.reshape(1, -1)  # 2D ë°°ì—´ ë³€í™˜
    faiss.normalize_L2(query_embedding)
    _, indices = index.search(query_embedding, top_k)

    faiss_movie_indices = [movie_indices[i] for i in indices[0]]
    faiss_results = df.iloc[faiss_movie_indices].drop_duplicates(subset=["ì˜í™” ì œëª©"])

    if not faiss_results.empty:
        faiss_results = filter_movies_by_exclusion(faiss_results, expanded_keywords["ì œì™¸í•  í‚¤ì›Œë“œ"])

    # ðŸ”¹ 4. ìµœì¢… ê²°ê³¼ ë³‘í•© ë° ì¤‘ë³µ ì œê±°
    result_df = pd.concat([result_df, core_keyword_results, faiss_results]).drop_duplicates(subset=["ì˜í™” ì œëª©"])

    print(f"[INFO] ìµœì¢… ê²€ìƒ‰ëœ ì˜í™” ê°œìˆ˜: {len(result_df)}")
    return result_df


# STEP3: LLMì„ í™œìš©í•œ ìµœì¢… ì¶”ì²œ ìƒì„±
def generate_recommendations(user_question, search_results, max_results=5, batch_size=10):
    if search_results.empty:
        return []

    movie_data = search_results[['ì˜í™” ì œëª©', 'ìž¥ë¥´', 'ê°ë…', 'ì£¼ì—°', 'í‚¤ì›Œë“œ(í•œê¸€)']].to_dict(orient='records')
    total_movies = len(movie_data)


    # LLM í˜¸ì¶œì„ Batch ë‹¨ìœ„ë¡œ ìˆ˜í–‰í•˜ì—¬ RateLimit ë°©ì§€
    recommended_movies = []
    for i in range(0, total_movies, batch_size):
        batch = movie_data[i:i+batch_size]

        prompt = f"""
        ë‹¹ì‹ ì€ ì˜í™”ë¥¼ ì¶”ì²œí•˜ëŠ” ì˜í™” ì¶”ì²œ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤.
        ì‚¬ìš©ìžê°€ ì˜í™” ì¶”ì²œì„ ìš”ì²­í–ˆìŠµë‹ˆë‹¤.
        ì§ˆë¬¸: "{user_question}"
        ì‚¬ìš©ìžê°€ ì›í•˜ëŠ” ì¶”ì²œ ê°œìˆ˜: {max_results}ê°œ
        
        ì•„ëž˜ëŠ” ê²€ìƒ‰ëœ ì˜í™” ëª©ë¡ìž…ë‹ˆë‹¤.
        ì´ ì¤‘ì—ì„œ ì‚¬ìš©ìžê°€ ìš”ì²­í•œ ì§ˆë¬¸ì— ê´€ë ¨ì—†ëŠ” ì˜í™”ëŠ” ì œì™¸í•˜ê³ , ê°€ìž¥ ì ì ˆí•œ ì˜í™”ë¥¼ ì¶”ì²œí•˜ì„¸ìš”.
        ê´€ë ¨ëœ ì˜í™”ê°€ {max_results}ê°œë³´ë‹¤ ì ë‹¤ë©´ ì ì€ ê°œìˆ˜ë§Œ ì¶”ì²œí•˜ì„¸ìš”.

        ì‚¬ëžŒë“¤ì—ê²Œ ì¸ì§€ë„ê°€ ë†’ê³  ê¾¸ì¤€ížˆ íšŒìžë˜ëŠ” ì˜í™”ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì¶”ì²œí•˜ì„¸ìš”.
        ì§ˆë¬¸ì´ íŠ¹ì • í…Œë§ˆì˜ ì˜í™”ë¥¼ ì›í•˜ê³  ìžˆë‹¤ë©´ ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¶”ì²œí•˜ì„¸ìš”.
        ë‹¤ë§Œ, contextì— ì—†ëŠ” ì˜í™”ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

        **ì¤‘ìš”:** ë‹µë³€ì€ **ë°˜ë“œì‹œ** ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì˜í™”ì œëª© ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ì„¸ìš”. 
        JSON ì´ì™¸ì˜ í…ìŠ¤íŠ¸ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

        [[ì¶œë ¥í˜•ì‹]] :
        {json.dumps(batch, ensure_ascii=False)}

        ì˜í™”ì˜ ìƒì„¸ ì •ë³´ë¥¼ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
        ì˜¤ì§ "ì¶”ì²œ ì˜í™”"ì˜ ì œëª©ë§Œ ë¦¬ìŠ¤íŠ¸ì— í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤. 

        ì˜ˆì‹œ:
        {{"ì¶”ì²œ ì˜í™”": ["ì˜í™”1", "ì˜í™”2", "ì˜í™”3"]}}
        """

        response = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "system", "content": "ë‹¹ì‹ ì€ ì˜í™” ì¶”ì²œ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}],
        )

        # ì‚­ì œí•œ í”„ë¡¬í”„íŠ¸ ë‚´ìš©: ë‹µë³€ì„ ë°˜í™˜í•˜ê¸° ì „, ë‹¤ì‹œ í•œë²ˆ ì´ ì˜í™”ê°€ ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ì–´ìš¸ë¦¬ëŠ” ì˜í™”ì¸ì§€, contextì— ìžˆëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”.

        try:
            batch_recommendations = json.loads(response.choices[0].message.content).get("ì¶”ì²œ ì˜í™”", [])

            # âœ… dict í˜•íƒœê°€ ìžˆëŠ” ê²½ìš°, 'ì˜í™” ì œëª©' ê°’ë§Œ ì¶”ì¶œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            batch_recommendations = [movie["ì˜í™” ì œëª©"] if isinstance(movie, dict) else movie for movie in batch_recommendations]

            # movie_list = []
            # for movie in batch_recommendations:
            #     if isinstance(movie, dict):
            #         movie_list.append(movie["ì˜í™” ì œëª©"])
            #         print(movie_list)
            #     else:
            #         movie_list.append(movie)
            #         print(movie_list)

            # recommended_movies = movie_list

            recommended_movies.extend(batch_recommendations)

        except json.JSONDecodeError:
            print("âŒ LLMì—ì„œ ìž˜ëª»ëœ ì‘ë‹µì„ ë°›ì•˜ìŠµë‹ˆë‹¤.")
            print(f"ì‘ë‹µë‚´ìš©: {response.choices[0].message.content}")

        # âœ… ìµœëŒ€ ê²°ê³¼ ê°œìˆ˜ë§Œí¼ë§Œ ìœ ì§€
        if len(recommended_movies) >= max_results:
            break

    return recommended_movies[:max_results]