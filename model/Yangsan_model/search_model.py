import pandas as pd
import faiss
import numpy as np
import json
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv
import openai
import os

load_dotenv()
hf_api_key = os.getenv("HUGGINGFACE_API_KEY")
openai_client = openai.Client(api_key=os.getenv("OPENAI_API_KEY"))

# ìž„ë² ë”© ëª¨ë¸ ë¡œë“œ
model = SentenceTransformer("all-MiniLM-L6-v2", use_auth_token=hf_api_key)

# ì—‘ì…€ DB ë¡œë“œ
EXCEL_FILE_PATH = "../data/MOVIE_DB/MOVIE_DB_7005.xlsx"
df = pd.read_excel(EXCEL_FILE_PATH).fillna("")
review_db_path = "../data/MOVIE_DB/Reviews.xlsx"
review_df = pd.read_excel(review_db_path)

# FAISS ì¸ë±ìŠ¤ ì €ìž¥ íŒŒì¼ ê²½ë¡œ
FAISS_INDEX_FILE = "../search_model/faiss_index.bin"
MOVIE_INDICES_FILE = "../search_model/movie_indices.npy"

#####################################################################
# ë‹¤ì¤‘ ë²¡í„°í™”
#####################################################################

def build_faiss_index():
    print("[INFO] ë‹¤ì¤‘ ë²¡í„° ê¸°ë°˜ FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")

    all_embeddings = []
    movie_mapping = []

    for idx, row in df.iterrows():
        embeddings_list = []

        # ðŸ”¹ "ìž¥ë¥´" ë²¡í„°í™” (ê° ìž¥ë¥´ ê°œë³„ ë²¡í„° ìƒì„±)
        if pd.notna(row['ìž¥ë¥´']):
            genres = row['ìž¥ë¥´'].split(", ")  # "ì•¡ì…˜, ë²”ì£„" â†’ ["ì•¡ì…˜", "ë²”ì£„"]
            genre_embeddings = model.encode(genres, convert_to_numpy=True)
            embeddings_list.append(genre_embeddings)

        # ðŸ”¹ "í‚¤ì›Œë“œ" ë²¡í„°í™” (ê° í‚¤ì›Œë“œ ê°œë³„ ë²¡í„° ìƒì„±)
        if pd.notna(row['í‚¤ì›Œë“œ(í•œê¸€)']):
            keywords = row['í‚¤ì›Œë“œ(í•œê¸€)'].split(", ")  # "íƒˆì¶œ, ë‚˜ì¹˜, ì—­ì‚¬" â†’ ["íƒˆì¶œ", "ë‚˜ì¹˜", "ì—­ì‚¬"]
            keyword_embeddings = model.encode(keywords, convert_to_numpy=True)
            embeddings_list.append(keyword_embeddings)

        # ðŸ”¹ "ì˜í™” ì†Œê°œ" ë²¡í„°í™” (ë¬¸ìž¥ ë‹¨ìœ„ ë²¡í„° ìƒì„±)
        if pd.notna(row['ì†Œê°œ']):
            intro_embedding = model.encode([row['ì†Œê°œ']], convert_to_numpy=True)
            embeddings_list.append(intro_embedding)

        # ðŸ”¹ ëª¨ë“  ë²¡í„°ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
        if embeddings_list:
            movie_embeddings = np.vstack(embeddings_list)
            all_embeddings.append(movie_embeddings)

            # ðŸ”¹ í•´ë‹¹ ì˜í™”ì˜ IDë¥¼ ì—¬ëŸ¬ ê°œ ì¶”ê°€ (ë²¡í„° ê°œìˆ˜ë§Œí¼)
            for _ in range(len(movie_embeddings)):
                movie_mapping.append(idx)

    # ðŸ”¹ ë²¡í„° ë°°ì—´ë¡œ ë³€í™˜ ë° ì •ê·œí™”
    all_embeddings = np.vstack(all_embeddings)
    faiss.normalize_L2(all_embeddings)

    # ðŸ”¹ FAISS ì¸ë±ìŠ¤ ìƒì„±
    d = all_embeddings.shape[1]  # ë²¡í„° ì°¨ì› ìˆ˜
    index = faiss.IndexFlatIP(d)
    index.add(all_embeddings)

    # ðŸ”¹ ì¸ë±ìŠ¤ ì €ìž¥
    faiss.write_index(index, FAISS_INDEX_FILE)
    np.save(MOVIE_INDICES_FILE, np.array(movie_mapping))

    print("[INFO] FAISS ì¸ë±ìŠ¤ ì €ìž¥ ì™„ë£Œ.")


# FAISS ì¸ë±ìŠ¤ ë¡œë“œ (ì—†ìœ¼ë©´ ìƒì„±)
def load_faiss_index():
    if os.path.exists(FAISS_INDEX_FILE) and os.path.exists(MOVIE_INDICES_FILE):
        print("[INFO] ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...")
        index = faiss.read_index(FAISS_INDEX_FILE)
        movie_indices = np.load(MOVIE_INDICES_FILE)
        return index, movie_indices
    else:
        print("âŒ FAISS ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤...")
        build_faiss_index()  # FAISS ì¸ë±ìŠ¤ ìƒì„±
        index = faiss.read_index(FAISS_INDEX_FILE)
        movie_indices = np.load(MOVIE_INDICES_FILE)
        return index, movie_indices
    

# STEP1 : ì‚¬ìš©ìž ì§ˆë¬¸ LLM ë¶„ì„ ì§„í–‰.
def analyze_question_with_llm(user_question):
    prompt = f"""
    ì‚¬ìš©ìžê°€ ì˜í™” ì¶”ì²œì„ ìš”ì²­í–ˆìŠµë‹ˆë‹¤.
    ë‹¤ìŒ ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì˜í™”ì˜ ìž¥ë¥´, ë¶„ìœ„ê¸°, í‚¤ì›Œë“œ, ê°ë…, ì£¼ì—° ë“±ì˜ ì •ë³´ë¥¼ ë¶„ì„í•˜ì„¸ìš”.
    ì§ˆë¬¸: "{user_question}"

    ì´ ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ê°€ìž¥ ì ì ˆí•œ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ì„¸ìš”.

    "í•µì‹¬ í‚¤ì›Œë“œ"ëŠ” ì§ˆë¬¸ì—ì„œ ê°€ìž¥ ì¤‘ìš”í•œ ìš”ì†Œì´ë©°, íŠ¹ì •í•œ ë°°ê²½, í…Œë§ˆ, ê³„ì ˆ, ê°ì„± ë“±ì„ ë‚˜íƒ€ë‚¼ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
    "í•µì‹¬ í‚¤ì›Œë“œ"ê°€ ì—†ëŠ” ê²½ìš°ì—ëŠ” ë¹ˆ ê°’ìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”.

    - ì˜ˆì œ:
        - "í¬ë¦¬ìŠ¤ë§ˆìŠ¤ ì˜í™” ì¶”ì²œí•´ì¤˜" â†’ í•µì‹¬ í‚¤ì›Œë“œ: ["í¬ë¦¬ìŠ¤ë§ˆìŠ¤"]
        - "ë¹„ ì˜¤ëŠ” ë‚  ë³¼ ì˜í™” ì¶”ì²œí•´ì¤˜" â†’ í•µì‹¬ í‚¤ì›Œë“œ: ["ë¹„"]
        - "ì—°ì¸ì´ëž‘ ë³¼ë§Œí•œ ì˜í™” ì¶”ì²œí•´ì¤˜" â†’ í•µì‹¬ í‚¤ì›Œë“œ: ["ë¡œë§¨ìŠ¤"]
        - "ìž¬ë¯¸ìžˆëŠ” ì˜í™” ì¶”ì²œí•´ì¤˜" â†’ í•µì‹¬ í‚¤ì›Œë“œ: []
        - "ê°€ì¡±ê³¼ ë³¼ ë§Œí•œ ì˜í™” ì¶”ì²œí•´ì¤˜" â†’ í•µì‹¬ í‚¤ì›Œë“œ: []
    
    "í•µì‹¬ í‚¤ì›Œë“œ"ëŠ” 1ê°œë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.

    JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”.
    ì˜ˆì‹œ: {{"ìž¥ë¥´": ["ë¡œë§¨ìŠ¤", "ì½”ë¯¸ë””", "ë“œë¼ë§ˆ"], "ë¶„ìœ„ê¸°": ["ê°ì„±ì ì¸", "ë”°ëœ»í•œ"], "í‚¤ì›Œë“œ": ["í¬ë¦¬ìŠ¤ë§ˆìŠ¤", "ê²¨ìš¸", "ëˆˆ", "ì—°ë§"], "ê°ë…": "ë´‰ì¤€í˜¸", "ì£¼ì—°": "ì†¡ê°•í˜¸", "ì œì™¸í•  í‚¤ì›Œë“œ": ["í­ë ¥", "ê°•ê°„", "ìž”ì¸í•œ"], "í•µì‹¬ í‚¤ì›Œë“œ" : ["í¬ë¦¬ìŠ¤ë§ˆìŠ¤"]}}
    
    ë¶„ì„ ê²°ê³¼ì— ëŒ€í•œ ìž¥ë¥´, ë¶„ìœ„ê¸°, í‚¤ì›Œë“œ, ì œì™¸í•  í‚¤ì›Œë“œëŠ” 3ê°œì”© ë°˜í™˜í•©ë‹ˆë‹¤.
    
    ì§ˆë¬¸ì„ ë¶„ì„í•œ ë’¤ì— í•„ìš”ì¹˜ ì•ŠëŠ” KEYê°’ì€ ë°˜í™˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    ì˜ˆì‹œ: "ë´‰ì¤€í˜¸ ê°ë…ì˜ ì˜í™” ì¶”ì²œí•´ì¤˜"ì™€ ê°™ì€ ì§ˆë¬¸ì˜ ê²½ìš°, "ìž¥ë¥´", "ë¶„ìœ„ê¸°", "í‚¤ì›Œë“œ", "ì£¼ì—°", "ì œì™¸í•  í‚¤ì›Œë“œ" ë“±ì˜ ìš”ì†Œê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë°˜ë©´ì— "ê²¨ìš¸ì— ë³¼ë§Œí•œ ì˜í™” ì¶”ì²œí•´ì¤˜"ì™€ ê°™ì€ ì§ˆë¬¸ì˜ ê²½ìš°, "ê°ë…", "ì£¼ì—°" ë“±ì˜ ìš”ì†Œê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    íŠ¹ì • ì‹œë¦¬ì¦ˆ ì˜í™” ì¶”ì²œì„ ì›í•  ê²½ìš°, ì‹œë¦¬ì¦ˆë§ˆë‹¤ ì˜í™” ê°ë…ê³¼ ë°°ìš°ê°€ ë³€ê²½ë˜ëŠ” ê²½ìš°ê°€ ìžˆê¸° ë•Œë¬¸ì— ì´ë¥¼ ê³ ë ¤í•´ì•¼í•©ë‹ˆë‹¤.
    """

    response = openai_client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "system", "content": "ë‹¹ì‹ ì€ ì˜í™” ì¶”ì²œ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤."},
                  {"role": "user", "content": prompt}],
    )
    # ðŸ”¹ ì‘ë‹µ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
    raw_content = response.choices[0].message.content
    print("[INFO] GPT ì‘ë‹µ:", raw_content)  # ë””ë²„ê¹…ìš© ì¶œë ¥

    # ë¹ˆ ì‘ë‹µ ì²´í¬
    if not raw_content:
        print("âŒ GPT ì‘ë‹µì´ ë¹„ì–´ ìžˆìŠµë‹ˆë‹¤.")
        return {}

    # JSON ì½”ë“œ ë¸”ë¡ ì œê±° (`json.loads()`ë¥¼ ì ìš©í•˜ê¸° ì „ ì •ë¦¬)
    if raw_content.startswith("```json"):
        raw_content = raw_content.strip("```json").strip("```").strip()

    try:
        parsed_json = json.loads(raw_content)  # ðŸ”¹ JSON ë³€í™˜
        return parsed_json
    except json.JSONDecodeError as e:
        print(f"âŒ JSON ë³€í™˜ ì‹¤íŒ¨: {e}")
        return {}


## ì œì™¸í•  í‚¤ì›Œë“œë¥¼ ê°€ì§€ê³  ìžˆëŠ” ì˜í™” í•„í„°ë§í•˜ëŠ” í•¨ìˆ˜
def filter_movies_by_exclusion(df, exclusion_keywords):
    if not exclusion_keywords:
        return df  # ì œì™¸í•  í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    mask = df.apply(lambda row: not any(excl in row["í‚¤ì›Œë“œ(í•œê¸€)"] for excl in exclusion_keywords), axis=1)
    return df[mask]


# STEP3 : FAISS ê²€ìƒ‰ í›„ í•„í„°ë§ ì§„í–‰ (í•µì‹¬ í‚¤ì›Œë“œ ë°˜ì˜ + ê°ë…, ì£¼ì—° í•„í„°ë§ ë³´ê°•)
def search_movies_with_keywords(expanded_keywords, top_k=100):
    """
    - ê°ë…ì´ë‚˜ ì£¼ì—° ë°°ìš°ê°€ ìžˆëŠ” ê²½ìš°: ì „ì²´ DBì—ì„œ ì§ì ‘ í•„í„°ë§ (FAISS ì‚¬ìš© X)
    - í•µì‹¬ í‚¤ì›Œë“œê°€ ìžˆëŠ” ê²½ìš°: í•´ë‹¹ í‚¤ì›Œë“œë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ê²€ìƒ‰ í›„ ê¸°ì¡´ ê²€ìƒ‰ ê²°ê³¼ì™€ ë³‘í•©
    """
    index, movie_indices = load_faiss_index()

    if index is None or movie_indices is None:
        return pd.DataFrame()  # ì¸ë±ìŠ¤ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë°ì´í„°í”„ë ˆìž„ ë°˜í™˜

    expanded_keywords.setdefault("ìž¥ë¥´", [])
    expanded_keywords.setdefault("ë¶„ìœ„ê¸°", [])
    expanded_keywords.setdefault("í‚¤ì›Œë“œ", [])
    expanded_keywords.setdefault("í•µì‹¬ í‚¤ì›Œë“œ", [])
    expanded_keywords.setdefault("ì œì™¸í•  í‚¤ì›Œë“œ", [])
    expanded_keywords.setdefault("ê°ë…", "")
    expanded_keywords.setdefault("ì£¼ì—°", "")

    # í•­ìƒ ì¡´ìž¬í•˜ëŠ” ë¹ˆ ë°ì´í„°í”„ë ˆìž„ ì„ ì–¸ (ì—ëŸ¬ ë°©ì§€)
    result_df = pd.DataFrame()

    # ðŸ”¹ 1. ê°ë…ì´ ëª…í™•ížˆ ì£¼ì–´ì§„ ê²½ìš° â†’ DBì—ì„œ ì§ì ‘ í•„í„°ë§
    if expanded_keywords["ê°ë…"]:
        result_df = df[df["ê°ë…"].str.contains(expanded_keywords["ê°ë…"], na=False)]
        result_df = filter_movies_by_exclusion(result_df, expanded_keywords["ì œì™¸í•  í‚¤ì›Œë“œ"])
        return result_df

    # ðŸ”¹ 2. ì£¼ì—° ë°°ìš°ê°€ ëª…í™•ížˆ ì£¼ì–´ì§„ ê²½ìš° â†’ DBì—ì„œ ì§ì ‘ í•„í„°ë§
    if expanded_keywords["ì£¼ì—°"]:
        result_df = df[df["ì£¼ì—°"].str.contains(expanded_keywords["ì£¼ì—°"], na=False)]
        result_df = filter_movies_by_exclusion(result_df, expanded_keywords["ì œì™¸í•  í‚¤ì›Œë“œ"])
        return result_df

    # ðŸ”¹ 3. í•µì‹¬ í‚¤ì›Œë“œê°€ ìžˆë‹¤ë©´, í•´ë‹¹ í‚¤ì›Œë“œë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ê²€ìƒ‰
    keyword_filter_movies = pd.DataFrame()
    if expanded_keywords["í•µì‹¬ í‚¤ì›Œë“œ"]:
        keyword_filter_movies = df[df["í‚¤ì›Œë“œ(í•œê¸€)"].str.contains("|".join(expanded_keywords["í•µì‹¬ í‚¤ì›Œë“œ"]), na=False)]

    core_keyword_results = pd.DataFrame()
    if expanded_keywords["í•µì‹¬ í‚¤ì›Œë“œ"]:
        core_query_embedding = model.encode([", ".join(expanded_keywords["í•µì‹¬ í‚¤ì›Œë“œ"])], convert_to_numpy=True)
        faiss.normalize_L2(core_query_embedding)
        _, core_indices = index.search(core_query_embedding, top_k // 2)  # í•µì‹¬ í‚¤ì›Œë“œëŠ” ë”°ë¡œ ê²€ìƒ‰
        core_keyword_results = df.iloc[movie_indices[core_indices[0]]].drop_duplicates(subset=["ì˜í™” ì œëª©"])

    # ðŸ”¹ 4. FAISS ê²€ìƒ‰ ìˆ˜í–‰ (ìž¥ë¥´ + í‚¤ì›Œë“œ + ë¶„ìœ„ê¸° í¬í•¨)
    query_text = expanded_keywords["ìž¥ë¥´"] + expanded_keywords["í‚¤ì›Œë“œ"] + expanded_keywords["ë¶„ìœ„ê¸°"]
    weighted_query_text = query_text + (expanded_keywords["í•µì‹¬ í‚¤ì›Œë“œ"] * 2)  # í•µì‹¬ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ 2ë°° ì ìš©
    query_embedding = model.encode([", ".join(weighted_query_text)], convert_to_numpy=True)

    faiss.normalize_L2(query_embedding)
    _, indices = index.search(query_embedding, top_k)

    faiss_results = df.iloc[movie_indices[indices[0]]].drop_duplicates(subset=["ì˜í™” ì œëª©"])
    faiss_results = filter_movies_by_exclusion(faiss_results, expanded_keywords["ì œì™¸í•  í‚¤ì›Œë“œ"])

    print("[INFO] FAISS ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")

    # ðŸ”¹ 5. ë¶„ìœ„ê¸° í‚¤ì›Œë“œ ê¸°ë°˜ FAISS ê²€ìƒ‰ ì¶”ê°€ (ê¸°ì¡´ì˜ DB ì§ì ‘ ê²€ìƒ‰ ì œê±°)
    mood_results = pd.DataFrame()
    if expanded_keywords["ë¶„ìœ„ê¸°"]:
        mood_query_embedding = model.encode([", ".join(expanded_keywords["ë¶„ìœ„ê¸°"])], convert_to_numpy=True)
        faiss.normalize_L2(mood_query_embedding)
        _, mood_indices = index.search(mood_query_embedding, top_k)
        mood_results = df.iloc[movie_indices[mood_indices[0]]].drop_duplicates(subset=["ì˜í™” ì œëª©"])


    # 6. ë¹ˆ ë°ì´í„°í”„ë ˆìž„ ë¬¸ì œ í•´ê²° â†’ í•­ìƒ ì¡´ìž¬í•˜ëŠ” result_dfì™€ ë³‘í•©
    result_df = pd.concat([result_df, keyword_filter_movies, mood_results, core_keyword_results, faiss_results]).drop_duplicates(subset=["ì˜í™” ì œëª©"])

    print(f"[INFO] ìµœì¢… ê²€ìƒ‰ëœ ì˜í™” ê°œìˆ˜: {len(result_df)}")
    return result_df



# STEP4: LLMì„ í™œìš©í•œ ìµœì¢… ì¶”ì²œ ìƒì„±
def generate_recommendations(user_question, search_results, max_results=5, batch_size=10):
    if search_results.empty:
        return []

    movie_data = search_results[['ì˜í™” ì œëª©', 'ìž¥ë¥´', 'ê°ë…', 'ì£¼ì—°', 'í‚¤ì›Œë“œ(í•œê¸€)']].to_dict(orient='records')
    total_movies = len(movie_data)


    # LLM í˜¸ì¶œì„ Batch ë‹¨ìœ„ë¡œ ìˆ˜í–‰í•˜ì—¬ RateLimit ë°©ì§€
    recommended_movies = []
    for i in range(0, total_movies, batch_size):
        batch = movie_data[i:i+batch_size]

        prompt = f"""
        ë‹¹ì‹ ì€ ì˜í™”ë¥¼ ì¶”ì²œí•˜ëŠ” ì˜í™” ì¶”ì²œ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤.
        ì‚¬ìš©ìžê°€ ì˜í™” ì¶”ì²œì„ ìš”ì²­í–ˆìŠµë‹ˆë‹¤.
        ì§ˆë¬¸: "{user_question}"
        ì‚¬ìš©ìžê°€ ì›í•˜ëŠ” ì¶”ì²œ ê°œìˆ˜: {max_results}ê°œ
        
        ì•„ëž˜ëŠ” ê²€ìƒ‰ëœ ì˜í™” ëª©ë¡ìž…ë‹ˆë‹¤.
        ì´ ì¤‘ì—ì„œ ì‚¬ìš©ìžê°€ ìš”ì²­í•œ ì§ˆë¬¸ì— ê´€ë ¨ì—†ëŠ” ì˜í™”ëŠ” ì œì™¸í•˜ê³ , ê°€ìž¥ ì ì ˆí•œ ì˜í™”ë¥¼ ì¶”ì²œí•˜ì„¸ìš”.
        ë‹¨, ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ì—†ëŠ” ì˜í™”ëŠ” ì œì™¸í•˜ê³ , ê´€ë ¨ëœ ì˜í™”ê°€ {max_results}ê°œë³´ë‹¤ ì ë‹¤ë©´ ì ì€ ê°œìˆ˜ë§Œ ì¶”ì²œí•˜ì„¸ìš”.

        ì‚¬ëžŒë“¤ì—ê²Œ ì¸ì§€ë„ê°€ ë†’ê³  ê¾¸ì¤€ížˆ íšŒìžë˜ëŠ” ì˜í™”ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì¶”ì²œí•˜ì„¸ìš”.
        ë‹¤ë§Œ, DBì— ì—†ëŠ” ì˜í™”ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

        ë‹µë³€ì„ ë°˜í™˜í•˜ê¸° ì „, ë‹¤ì‹œ í•œë²ˆ ì´ ì˜í™”ê°€ ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ì–´ìš¸ë¦¬ëŠ” ì˜í™”ì¸ì§€ íŒë‹¨í•˜ì„¸ìš”.

        JSON í˜•ì‹ìœ¼ë¡œ ì˜í™” ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ì„¸ìš”. JSON ì´ì™¸ì˜ í…ìŠ¤íŠ¸ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
        {json.dumps(batch, ensure_ascii=False)}

        ì˜ˆì‹œ:
        {{"ì¶”ì²œ ì˜í™”": ["ì˜í™”1", "ì˜í™”2", "ì˜í™”3"]}}
        """

        response = openai_client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "system", "content": "ë‹¹ì‹ ì€ ì˜í™” ì¶”ì²œ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}],
        )

        try:
            batch_recommendations = json.loads(response.choices[0].message.content).get("ì¶”ì²œ ì˜í™”", [])
            recommended_movies.extend(batch_recommendations)
        except json.JSONDecodeError:
            print("âŒ LLMì—ì„œ ìž˜ëª»ëœ ì‘ë‹µì„ ë°›ì•˜ìŠµë‹ˆë‹¤.")

                # âœ… ìµœëŒ€ ê²°ê³¼ ê°œìˆ˜ë§Œí¼ë§Œ ìœ ì§€
        if len(recommended_movies) >= max_results:
            break

    return recommended_movies[:max_results]

