{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai==0.28\n",
      "  Using cached openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\playdata\\miniconda3\\envs\\pjt\\lib\\site-packages (from openai==0.28) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\playdata\\miniconda3\\envs\\pjt\\lib\\site-packages (from openai==0.28) (4.67.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\playdata\\miniconda3\\envs\\pjt\\lib\\site-packages (from openai==0.28) (3.11.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\playdata\\miniconda3\\envs\\pjt\\lib\\site-packages (from requests>=2.20->openai==0.28) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\playdata\\miniconda3\\envs\\pjt\\lib\\site-packages (from requests>=2.20->openai==0.28) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\playdata\\miniconda3\\envs\\pjt\\lib\\site-packages (from requests>=2.20->openai==0.28) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\playdata\\miniconda3\\envs\\pjt\\lib\\site-packages (from requests>=2.20->openai==0.28) (2024.12.14)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\playdata\\miniconda3\\envs\\pjt\\lib\\site-packages (from aiohttp->openai==0.28) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\playdata\\miniconda3\\envs\\pjt\\lib\\site-packages (from aiohttp->openai==0.28) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\playdata\\miniconda3\\envs\\pjt\\lib\\site-packages (from aiohttp->openai==0.28) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\playdata\\miniconda3\\envs\\pjt\\lib\\site-packages (from aiohttp->openai==0.28) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\playdata\\miniconda3\\envs\\pjt\\lib\\site-packages (from aiohttp->openai==0.28) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\playdata\\miniconda3\\envs\\pjt\\lib\\site-packages (from aiohttp->openai==0.28) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\playdata\\miniconda3\\envs\\pjt\\lib\\site-packages (from aiohttp->openai==0.28) (1.18.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\playdata\\miniconda3\\envs\\pjt\\lib\\site-packages (from tqdm->openai==0.28) (0.4.6)\n",
      "Using cached openai-0.28.0-py3-none-any.whl (76 kB)\n",
      "Installing collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.61.0\n",
      "    Uninstalling openai-1.61.0:\n",
      "      Successfully uninstalled openai-1.61.0\n",
      "Successfully installed openai-0.28.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-openai 0.3.3 requires openai<2.0.0,>=1.58.1, but you have openai 0.28.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# %pip install -q sentence-transformers\n",
    "# %pip install tf-keras\n",
    "# %pip install konlpy\n",
    "# %pip install python-dotenv\n",
    "# %pip install openai==0.28\n",
    "# %pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\miniconda3\\envs\\pjt\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:195: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v4 of SentenceTransformers.\n",
      "  \"The `use_auth_token` argument is deprecated and will be removed in v4 of SentenceTransformers.\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] 사용자 질문 분석 중...\n",
      "[INFO] GPT 응답: ```json\n",
      "{\n",
      "    \"장르\": [\"판타지\", \"로맨스\", \"어드벤처\"],\n",
      "    \"분위기\": [\"동화 같은\", \"환상적인\", \"모험적인\"],\n",
      "    \"키워드\": [\"공주\", \"마법\", \"왕국\"],\n",
      "    \"제외할 키워드\": [\"폭력\", \"잔인한\", \"비극적인\"],\n",
      "    \"핵심 키워드\": [\"공주\"]\n",
      "}\n",
      "```\n",
      "[INFO] 분석된 검색 키워드: {'장르': ['판타지', '로맨스', '어드벤처'], '분위기': ['동화 같은', '환상적인', '모험적인'], '키워드': ['공주', '마법', '왕국'], '제외할 키워드': ['폭력', '잔인한', '비극적인'], '핵심 키워드': ['공주']}\n",
      "\n",
      "[INFO] 영화 검색 진행 중...\n",
      "[INFO] 기존 FAISS 인덱스 로드 중...\n",
      "[INFO] FAISS 검색 결과를 가져왔습니다.\n",
      "[INFO] 최종 검색된 영화 개수: 223\n",
      "\n",
      "[INFO] 추천 영화 선택 중...\n",
      "\n",
      "🎬 최종 추천 영화 리스트:\n",
      "['포카혼타스', '뮬란', '개미', '프린세스 다이어리1', '슈렉 2', '판의 미로-오필리아와 세 개의 열쇠', '벼랑 위의 포뇨']\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "# STEP1 : LLM이 사용자 질문 분석\n",
    "# STEP2 : FAISS 전달 및 필터링\n",
    "# STEP3: 필터링된 영화를 바탕으로 추천 진행\n",
    "###########################################\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import numpy as np\n",
    "import openai\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# API 키 설정\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "hf_api_key = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "\n",
    "# 엑셀 DB 로드\n",
    "EXCEL_FILE_PATH = \"../data/MOVIE_DB/MOVIE_DB_7005.xlsx\"\n",
    "df = pd.read_excel(EXCEL_FILE_PATH).fillna(\"\")\n",
    "\n",
    "# FAISS 인덱스 저장 파일 경로\n",
    "FAISS_INDEX_FILE = \"../data/MOVIE_DB/faiss_index.bin\"\n",
    "MOVIE_INDICES_FILE = \"../data/MOVIE_DB/movie_indices.npy\"\n",
    "\n",
    "# 임베딩 모델 로드\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", use_auth_token=hf_api_key)\n",
    "\n",
    "#####################################################################\n",
    "# 다중 벡터화\n",
    "#####################################################################\n",
    "\n",
    "def build_faiss_index():\n",
    "    print(\"[INFO] 다중 벡터 기반 FAISS 인덱스 구축 중...\")\n",
    "\n",
    "    all_embeddings = []\n",
    "    movie_mapping = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        embeddings_list = []\n",
    "\n",
    "        # 🔹 \"장르\" 벡터화 (각 장르 개별 벡터 생성)\n",
    "        if pd.notna(row['장르']):\n",
    "            genres = row['장르'].split(\", \")  # \"액션, 범죄\" → [\"액션\", \"범죄\"]\n",
    "            genre_embeddings = model.encode(genres, convert_to_numpy=True)\n",
    "            embeddings_list.append(genre_embeddings)\n",
    "\n",
    "        # 🔹 \"키워드\" 벡터화 (각 키워드 개별 벡터 생성)\n",
    "        if pd.notna(row['키워드(한글)']):\n",
    "            keywords = row['키워드(한글)'].split(\", \")  # \"탈출, 나치, 역사\" → [\"탈출\", \"나치\", \"역사\"]\n",
    "            keyword_embeddings = model.encode(keywords, convert_to_numpy=True)\n",
    "            embeddings_list.append(keyword_embeddings)\n",
    "\n",
    "        # 🔹 \"영화 소개\" 벡터화 (문장 단위 벡터 생성)\n",
    "        if pd.notna(row['소개']):\n",
    "            intro_embedding = model.encode([row['소개']], convert_to_numpy=True)\n",
    "            embeddings_list.append(intro_embedding)\n",
    "\n",
    "        # 🔹 모든 벡터를 하나로 합치기\n",
    "        if embeddings_list:\n",
    "            movie_embeddings = np.vstack(embeddings_list)\n",
    "            all_embeddings.append(movie_embeddings)\n",
    "\n",
    "            # 🔹 해당 영화의 ID를 여러 개 추가 (벡터 개수만큼)\n",
    "            for _ in range(len(movie_embeddings)):\n",
    "                movie_mapping.append(idx)\n",
    "\n",
    "    # 🔹 벡터 배열로 변환 및 정규화\n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    faiss.normalize_L2(all_embeddings)\n",
    "\n",
    "    # 🔹 FAISS 인덱스 생성\n",
    "    d = all_embeddings.shape[1]  # 벡터 차원 수\n",
    "    index = faiss.IndexFlatIP(d)\n",
    "    index.add(all_embeddings)\n",
    "\n",
    "    # 🔹 인덱스 저장\n",
    "    faiss.write_index(index, FAISS_INDEX_FILE)\n",
    "    np.save(MOVIE_INDICES_FILE, np.array(movie_mapping))\n",
    "\n",
    "    print(\"[INFO] FAISS 인덱스 저장 완료.\")\n",
    "\n",
    "\n",
    "# FAISS 인덱스 로드 (없으면 생성)\n",
    "def load_faiss_index():\n",
    "    if os.path.exists(FAISS_INDEX_FILE) and os.path.exists(MOVIE_INDICES_FILE):\n",
    "        print(\"[INFO] 기존 FAISS 인덱스 로드 중...\")\n",
    "        index = faiss.read_index(FAISS_INDEX_FILE)\n",
    "        movie_indices = np.load(MOVIE_INDICES_FILE)\n",
    "        return index, movie_indices\n",
    "    else:\n",
    "        print(\"❌ FAISS 인덱스를 찾을 수 없습니다. 새로 생성합니다...\")\n",
    "        build_faiss_index()  # FAISS 인덱스 생성\n",
    "        index = faiss.read_index(FAISS_INDEX_FILE)\n",
    "        movie_indices = np.load(MOVIE_INDICES_FILE)\n",
    "        return index, movie_indices\n",
    "    \n",
    "\n",
    "# STEP1 : 사용자 질문 LLM 분석 진행.\n",
    "def analyze_question_with_llm(question):\n",
    "    prompt = f\"\"\"\n",
    "    사용자가 영화 추천을 요청했습니다.\n",
    "    다음 질문을 기반으로 영화의 장르, 분위기, 키워드, 감독, 주연 등의 정보를 분석하세요.\n",
    "    질문: \"{question}\"\n",
    "\n",
    "    이 질문을 기반으로 가장 적절한 검색 키워드를 생성하세요.\n",
    "\n",
    "    \"핵심 키워드\"는 질문에서 가장 중요한 요소이며, 특정한 배경, 테마, 계절, 감성 등을 나타낼 수 있습니다.\n",
    "    \"핵심 키워드\"가 없는 경우에는 빈 값으로 설정하세요.\n",
    "\n",
    "    - 예제:\n",
    "        - \"크리스마스 영화 추천해줘\" → 핵심 키워드: [\"크리스마스\"]\n",
    "        - \"비 오는 날 볼 영화 추천해줘\" → 핵심 키워드: [\"비\"]\n",
    "        - \"연인이랑 볼만한 영화 추천해줘\" → 핵심 키워드: [\"로맨스\"]\n",
    "        - \"재미있는 영화 추천해줘\" → 핵심 키워드: []\n",
    "        - \"가족과 볼 만한 영화 추천해줘\" → 핵심 키워드: []\n",
    "    \n",
    "    \"핵심 키워드\"는 1개만 반환합니다.\n",
    "\n",
    "    JSON 형식으로 반환하세요.\n",
    "    예시: {{\"장르\": [\"로맨스\", \"코미디\", \"드라마\"], \"분위기\": [\"감성적인\", \"따뜻한\"], \"키워드\": [\"크리스마스\", \"겨울\", \"눈\", \"연말\"], \"감독\": \"봉준호\", \"주연\": \"송강호\", \"제외할 키워드\": [\"폭력\", \"강간\", \"잔인한\"], \"핵심 키워드\" : [\"크리스마스\"]}}\n",
    "    \n",
    "    분석 결과에 대한 장르, 분위기, 키워드, 제외할 키워드는 3개씩 반환합니다.\n",
    "    \n",
    "    질문을 분석한 뒤에 필요치 않는 KEY값은 반환하지 않습니다.\n",
    "    예시: \"봉준호 감독의 영화 추천해줘\"와 같은 질문의 경우, \"장르\", \"분위기\", \"키워드\", \"주연\", \"제외할 키워드\" 등의 요소가 필요하지 않습니다. 반면에 \"겨울에 볼만한 영화 추천해줘\"와 같은 질문의 경우, \"감독\", \"주연\" 등의 요소가 필요하지 않습니다.\n",
    "    \"\"\"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"당신은 영화 추천 전문가입니다.\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "    # 🔹 응답 내용 가져오기\n",
    "    raw_content = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    print(\"[INFO] GPT 응답:\", raw_content)  # 디버깅용 출력\n",
    "\n",
    "    # 🔹 JSON 코드 블록이 포함되어 있는 경우 처리\n",
    "    if raw_content.startswith(\"```json\"):\n",
    "        raw_content = raw_content.strip(\"```json\").strip(\"```\")  # ```json 제거\n",
    "\n",
    "    try:\n",
    "        parsed_json = json.loads(raw_content)\n",
    "        return parsed_json\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"❌ JSON 변환 실패:\", e)\n",
    "        return {}\n",
    "\n",
    "\n",
    "## 제외할 키워드를 가지고 있는 영화 필터링하는 함수\n",
    "def filter_movies_by_exclusion(df, exclusion_keywords):\n",
    "    if not exclusion_keywords:\n",
    "        return df  # 제외할 키워드가 없으면 그대로 반환\n",
    "\n",
    "    mask = df.apply(lambda row: not any(excl in row[\"키워드(한글)\"] for excl in exclusion_keywords), axis=1)\n",
    "    return df[mask]\n",
    "\n",
    "\n",
    "# STEP3 : FAISS 검색 후 필터링 진행 (핵심 키워드 반영 + 감독, 주연 필터링 보강)\n",
    "def search_movies_with_keywords(expanded_keywords, top_k=100):\n",
    "    \"\"\"\n",
    "    - 감독이나 주연 배우가 있는 경우: 전체 DB에서 직접 필터링 (FAISS 사용 X)\n",
    "    - 핵심 키워드가 있는 경우: 해당 키워드를 최우선으로 검색 후 기존 검색 결과와 병합\n",
    "    \"\"\"\n",
    "    index, movie_indices = load_faiss_index()\n",
    "\n",
    "    if index is None or movie_indices is None:\n",
    "        return pd.DataFrame()  # 인덱스가 없으면 빈 데이터프레임 반환\n",
    "\n",
    "    expanded_keywords.setdefault(\"장르\", [])\n",
    "    expanded_keywords.setdefault(\"분위기\", [])\n",
    "    expanded_keywords.setdefault(\"키워드\", [])\n",
    "    expanded_keywords.setdefault(\"핵심 키워드\", [])\n",
    "    expanded_keywords.setdefault(\"제외할 키워드\", [])\n",
    "    expanded_keywords.setdefault(\"감독\", \"\")\n",
    "    expanded_keywords.setdefault(\"주연\", \"\")\n",
    "\n",
    "    # ✅ 항상 존재하는 빈 데이터프레임 선언 (에러 방지)\n",
    "    result_df = pd.DataFrame()\n",
    "\n",
    "    # 🔹 1. 감독이 명확히 주어진 경우 → DB에서 직접 필터링\n",
    "    if expanded_keywords[\"감독\"]:\n",
    "        result_df = df[df[\"감독\"].str.contains(expanded_keywords[\"감독\"], na=False)]\n",
    "        result_df = filter_movies_by_exclusion(result_df, expanded_keywords[\"제외할 키워드\"])\n",
    "        return result_df\n",
    "\n",
    "    # 🔹 2. 주연 배우가 명확히 주어진 경우 → DB에서 직접 필터링\n",
    "    if expanded_keywords[\"주연\"]:\n",
    "        result_df = df[df[\"주연\"].str.contains(expanded_keywords[\"주연\"], na=False)]\n",
    "        result_df = filter_movies_by_exclusion(result_df, expanded_keywords[\"제외할 키워드\"])\n",
    "        return result_df\n",
    "\n",
    "    # 🔹 3. 핵심 키워드가 있다면, 해당 키워드를 최우선으로 검색\n",
    "    keyword_filter_movies = pd.DataFrame()\n",
    "    if expanded_keywords[\"핵심 키워드\"]:\n",
    "        keyword_filter_movies = df[df[\"키워드(한글)\"].str.contains(\"|\".join(expanded_keywords[\"핵심 키워드\"]), na=False)]\n",
    "\n",
    "    core_keyword_results = pd.DataFrame()\n",
    "    if expanded_keywords[\"핵심 키워드\"]:\n",
    "        core_query_embedding = model.encode([\", \".join(expanded_keywords[\"핵심 키워드\"])], convert_to_numpy=True)\n",
    "        faiss.normalize_L2(core_query_embedding)\n",
    "        _, core_indices = index.search(core_query_embedding, top_k // 2)  # 핵심 키워드는 따로 검색\n",
    "        core_keyword_results = df.iloc[movie_indices[core_indices[0]]].drop_duplicates(subset=[\"영화 제목\"])\n",
    "\n",
    "    # 🔹 4. FAISS 검색 수행 (장르 + 키워드 + 분위기 포함)\n",
    "    query_text = expanded_keywords[\"장르\"] + expanded_keywords[\"키워드\"] + expanded_keywords[\"분위기\"]\n",
    "    weighted_query_text = query_text + (expanded_keywords[\"핵심 키워드\"] * 2)  # 핵심 키워드 가중치 2배 적용\n",
    "    query_embedding = model.encode([\", \".join(weighted_query_text)], convert_to_numpy=True)\n",
    "\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    _, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    faiss_results = df.iloc[movie_indices[indices[0]]].drop_duplicates(subset=[\"영화 제목\"])\n",
    "    faiss_results = filter_movies_by_exclusion(faiss_results, expanded_keywords[\"제외할 키워드\"])\n",
    "\n",
    "    print(\"[INFO] FAISS 검색 결과를 가져왔습니다.\")\n",
    "\n",
    "    # 🔹 5. 분위기 키워드 기반 FAISS 검색 추가 (기존의 DB 직접 검색 제거)\n",
    "    mood_results = pd.DataFrame()\n",
    "    if expanded_keywords[\"분위기\"]:\n",
    "        mood_query_embedding = model.encode([\", \".join(expanded_keywords[\"분위기\"])], convert_to_numpy=True)\n",
    "        faiss.normalize_L2(mood_query_embedding)\n",
    "        _, mood_indices = index.search(mood_query_embedding, top_k)\n",
    "        mood_results = df.iloc[movie_indices[mood_indices[0]]].drop_duplicates(subset=[\"영화 제목\"])\n",
    "\n",
    "\n",
    "    # ✅ 6. 빈 데이터프레임 문제 해결 → 항상 존재하는 result_df와 병합\n",
    "    result_df = pd.concat([result_df, keyword_filter_movies, mood_results, core_keyword_results, faiss_results]).drop_duplicates(subset=[\"영화 제목\"])\n",
    "\n",
    "    print(f\"[INFO] 최종 검색된 영화 개수: {len(result_df)}\")\n",
    "    return result_df\n",
    "\n",
    "\n",
    "\n",
    "# STEP4: LLM을 활용한 최종 추천 생성\n",
    "def generate_recommendations(question, search_results, max_results=5, batch_size=10):\n",
    "    if search_results.empty:\n",
    "        return []\n",
    "\n",
    "    movie_data = search_results[['영화 제목', '장르', '감독', '주연', '키워드(한글)']].to_dict(orient='records')\n",
    "    total_movies = len(movie_data)\n",
    "\n",
    "\n",
    "    # ✅ LLM 호출을 Batch 단위로 수행하여 RateLimit 방지\n",
    "    recommended_movies = []\n",
    "    for i in range(0, total_movies, batch_size):\n",
    "        batch = movie_data[i:i+batch_size]\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        당신은 영화를 추천하는 영화 추천 전문가입니다.\n",
    "        사용자가 영화 추천을 요청했습니다.\n",
    "        질문: \"{question}\"\n",
    "        사용자가 원하는 추천 개수: {max_results}개\n",
    "        \n",
    "        아래는 검색된 영화 목록입니다.\n",
    "        이 중에서 사용자가 요청한 질문에 관련없는 영화는 제외하고, 가장 적절한 영화를 추천하세요.\n",
    "        단, 질문과 관련이 없는 영화는 제외하고, 관련된 영화가 {max_results}개보다 적다면 적은 개수만 추천하세요.\n",
    "\n",
    "        사람들에게 인지도가 높고 꾸준히 회자되는 영화를 우선적으로 추천하세요.\n",
    "        다만, DB에 없는 영화는 포함하지 마세요.\n",
    "\n",
    "        답변을 반환하기 전, 다시 한번 이 영화가 사용자의 질문에 어울리는 영화인지 판단하세요.\n",
    "\n",
    "        JSON 형식으로 영화 리스트를 반환하세요.\n",
    "        {json.dumps(batch, ensure_ascii=False)}\n",
    "\n",
    "        예시:\n",
    "        {{\"추천 영화\": [\"영화1\", \"영화2\", \"영화3\"]}}\n",
    "        \"\"\"\n",
    "\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[{\"role\": \"system\", \"content\": \"당신은 영화 추천 전문가입니다.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}],\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            batch_recommendations = json.loads(response[\"choices\"][0][\"message\"][\"content\"]).get(\"추천 영화\", [])\n",
    "            recommended_movies.extend(batch_recommendations)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"❌ LLM에서 잘못된 응답을 받았습니다.\")\n",
    "\n",
    "        # ✅ 최대 결과 개수만큼만 유지\n",
    "        if len(recommended_movies) >= max_results:\n",
    "            break\n",
    "\n",
    "    return recommended_movies[:max_results]\n",
    "\n",
    "# 실행\n",
    "if __name__ == \"__main__\":\n",
    "    question = input(\"🎬 영화 추천 질문을 입력하세요: \")\n",
    "    \n",
    "    max_results = int(input(\"🔢 추천받을 영화 개수를 입력하세요 (예: 5): \"))\n",
    "\n",
    "    print(\"\\n[INFO] 사용자 질문 분석 중...\")\n",
    "    expanded_keywords = analyze_question_with_llm(question)\n",
    "    print(\"[INFO] 분석된 검색 키워드:\", expanded_keywords)\n",
    "\n",
    "    print(\"\\n[INFO] 영화 검색 진행 중...\")\n",
    "    search_results = search_movies_with_keywords(expanded_keywords)\n",
    "\n",
    "    print(\"\\n[INFO] 추천 영화 선택 중...\")\n",
    "    movie_list = generate_recommendations(question, search_results, max_results=max_results)\n",
    "\n",
    "    print(\"\\n🎬 최종 추천 영화 리스트:\")\n",
    "    print(movie_list)  # 리스트 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['포카혼타스', '뮬란', '개미', '프린세스 다이어리1', '슈렉 2', '판의 미로-오필리아와 세 개의 열쇠', '벼랑 위의 포뇨']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pjt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
