{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai==0.28\n",
      "  Using cached openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\playdata\\miniconda3\\envs\\pjt\\lib\\site-packages (from openai==0.28) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\playdata\\miniconda3\\envs\\pjt\\lib\\site-packages (from openai==0.28) (4.67.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\playdata\\miniconda3\\envs\\pjt\\lib\\site-packages (from openai==0.28) (3.11.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\playdata\\miniconda3\\envs\\pjt\\lib\\site-packages (from requests>=2.20->openai==0.28) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\playdata\\miniconda3\\envs\\pjt\\lib\\site-packages (from requests>=2.20->openai==0.28) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\playdata\\miniconda3\\envs\\pjt\\lib\\site-packages (from requests>=2.20->openai==0.28) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\playdata\\miniconda3\\envs\\pjt\\lib\\site-packages (from requests>=2.20->openai==0.28) (2024.12.14)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\playdata\\miniconda3\\envs\\pjt\\lib\\site-packages (from aiohttp->openai==0.28) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\playdata\\miniconda3\\envs\\pjt\\lib\\site-packages (from aiohttp->openai==0.28) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\playdata\\miniconda3\\envs\\pjt\\lib\\site-packages (from aiohttp->openai==0.28) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\playdata\\miniconda3\\envs\\pjt\\lib\\site-packages (from aiohttp->openai==0.28) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\playdata\\miniconda3\\envs\\pjt\\lib\\site-packages (from aiohttp->openai==0.28) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\playdata\\miniconda3\\envs\\pjt\\lib\\site-packages (from aiohttp->openai==0.28) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\playdata\\miniconda3\\envs\\pjt\\lib\\site-packages (from aiohttp->openai==0.28) (1.18.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\playdata\\miniconda3\\envs\\pjt\\lib\\site-packages (from tqdm->openai==0.28) (0.4.6)\n",
      "Using cached openai-0.28.0-py3-none-any.whl (76 kB)\n",
      "Installing collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.61.0\n",
      "    Uninstalling openai-1.61.0:\n",
      "      Successfully uninstalled openai-1.61.0\n",
      "Successfully installed openai-0.28.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-openai 0.3.3 requires openai<2.0.0,>=1.58.1, but you have openai 0.28.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# %pip install -q sentence-transformers\n",
    "# %pip install tf-keras\n",
    "# %pip install konlpy\n",
    "# %pip install python-dotenv\n",
    "# %pip install openai==0.28\n",
    "# %pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\miniconda3\\envs\\pjt\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:195: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v4 of SentenceTransformers.\n",
      "  \"The `use_auth_token` argument is deprecated and will be removed in v4 of SentenceTransformers.\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] ì‚¬ìš©ì ì§ˆë¬¸ ë¶„ì„ ì¤‘...\n",
      "[INFO] GPT ì‘ë‹µ: ```json\n",
      "{\n",
      "    \"ì¥ë¥´\": [\"íŒíƒ€ì§€\", \"ë¡œë§¨ìŠ¤\", \"ì–´ë“œë²¤ì²˜\"],\n",
      "    \"ë¶„ìœ„ê¸°\": [\"ë™í™” ê°™ì€\", \"í™˜ìƒì ì¸\", \"ëª¨í—˜ì ì¸\"],\n",
      "    \"í‚¤ì›Œë“œ\": [\"ê³µì£¼\", \"ë§ˆë²•\", \"ì™•êµ­\"],\n",
      "    \"ì œì™¸í•  í‚¤ì›Œë“œ\": [\"í­ë ¥\", \"ì”ì¸í•œ\", \"ë¹„ê·¹ì ì¸\"],\n",
      "    \"í•µì‹¬ í‚¤ì›Œë“œ\": [\"ê³µì£¼\"]\n",
      "}\n",
      "```\n",
      "[INFO] ë¶„ì„ëœ ê²€ìƒ‰ í‚¤ì›Œë“œ: {'ì¥ë¥´': ['íŒíƒ€ì§€', 'ë¡œë§¨ìŠ¤', 'ì–´ë“œë²¤ì²˜'], 'ë¶„ìœ„ê¸°': ['ë™í™” ê°™ì€', 'í™˜ìƒì ì¸', 'ëª¨í—˜ì ì¸'], 'í‚¤ì›Œë“œ': ['ê³µì£¼', 'ë§ˆë²•', 'ì™•êµ­'], 'ì œì™¸í•  í‚¤ì›Œë“œ': ['í­ë ¥', 'ì”ì¸í•œ', 'ë¹„ê·¹ì ì¸'], 'í•µì‹¬ í‚¤ì›Œë“œ': ['ê³µì£¼']}\n",
      "\n",
      "[INFO] ì˜í™” ê²€ìƒ‰ ì§„í–‰ ì¤‘...\n",
      "[INFO] ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...\n",
      "[INFO] FAISS ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.\n",
      "[INFO] ìµœì¢… ê²€ìƒ‰ëœ ì˜í™” ê°œìˆ˜: 223\n",
      "\n",
      "[INFO] ì¶”ì²œ ì˜í™” ì„ íƒ ì¤‘...\n",
      "\n",
      "ğŸ¬ ìµœì¢… ì¶”ì²œ ì˜í™” ë¦¬ìŠ¤íŠ¸:\n",
      "['í¬ì¹´í˜¼íƒ€ìŠ¤', 'ë®¬ë€', 'ê°œë¯¸', 'í”„ë¦°ì„¸ìŠ¤ ë‹¤ì´ì–´ë¦¬1', 'ìŠˆë ‰ 2', 'íŒì˜ ë¯¸ë¡œ-ì˜¤í•„ë¦¬ì•„ì™€ ì„¸ ê°œì˜ ì—´ì‡ ', 'ë²¼ë‘ ìœ„ì˜ í¬ë‡¨']\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "# STEP1 : LLMì´ ì‚¬ìš©ì ì§ˆë¬¸ ë¶„ì„\n",
    "# STEP2 : FAISS ì „ë‹¬ ë° í•„í„°ë§\n",
    "# STEP3: í•„í„°ë§ëœ ì˜í™”ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¶”ì²œ ì§„í–‰\n",
    "###########################################\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import numpy as np\n",
    "import openai\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# API í‚¤ ì„¤ì •\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "hf_api_key = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "\n",
    "# ì—‘ì…€ DB ë¡œë“œ\n",
    "EXCEL_FILE_PATH = \"../data/MOVIE_DB/MOVIE_DB_7005.xlsx\"\n",
    "df = pd.read_excel(EXCEL_FILE_PATH).fillna(\"\")\n",
    "\n",
    "# FAISS ì¸ë±ìŠ¤ ì €ì¥ íŒŒì¼ ê²½ë¡œ\n",
    "FAISS_INDEX_FILE = \"../data/MOVIE_DB/faiss_index.bin\"\n",
    "MOVIE_INDICES_FILE = \"../data/MOVIE_DB/movie_indices.npy\"\n",
    "\n",
    "# ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", use_auth_token=hf_api_key)\n",
    "\n",
    "#####################################################################\n",
    "# ë‹¤ì¤‘ ë²¡í„°í™”\n",
    "#####################################################################\n",
    "\n",
    "def build_faiss_index():\n",
    "    print(\"[INFO] ë‹¤ì¤‘ ë²¡í„° ê¸°ë°˜ FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...\")\n",
    "\n",
    "    all_embeddings = []\n",
    "    movie_mapping = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        embeddings_list = []\n",
    "\n",
    "        # ğŸ”¹ \"ì¥ë¥´\" ë²¡í„°í™” (ê° ì¥ë¥´ ê°œë³„ ë²¡í„° ìƒì„±)\n",
    "        if pd.notna(row['ì¥ë¥´']):\n",
    "            genres = row['ì¥ë¥´'].split(\", \")  # \"ì•¡ì…˜, ë²”ì£„\" â†’ [\"ì•¡ì…˜\", \"ë²”ì£„\"]\n",
    "            genre_embeddings = model.encode(genres, convert_to_numpy=True)\n",
    "            embeddings_list.append(genre_embeddings)\n",
    "\n",
    "        # ğŸ”¹ \"í‚¤ì›Œë“œ\" ë²¡í„°í™” (ê° í‚¤ì›Œë“œ ê°œë³„ ë²¡í„° ìƒì„±)\n",
    "        if pd.notna(row['í‚¤ì›Œë“œ(í•œê¸€)']):\n",
    "            keywords = row['í‚¤ì›Œë“œ(í•œê¸€)'].split(\", \")  # \"íƒˆì¶œ, ë‚˜ì¹˜, ì—­ì‚¬\" â†’ [\"íƒˆì¶œ\", \"ë‚˜ì¹˜\", \"ì—­ì‚¬\"]\n",
    "            keyword_embeddings = model.encode(keywords, convert_to_numpy=True)\n",
    "            embeddings_list.append(keyword_embeddings)\n",
    "\n",
    "        # ğŸ”¹ \"ì˜í™” ì†Œê°œ\" ë²¡í„°í™” (ë¬¸ì¥ ë‹¨ìœ„ ë²¡í„° ìƒì„±)\n",
    "        if pd.notna(row['ì†Œê°œ']):\n",
    "            intro_embedding = model.encode([row['ì†Œê°œ']], convert_to_numpy=True)\n",
    "            embeddings_list.append(intro_embedding)\n",
    "\n",
    "        # ğŸ”¹ ëª¨ë“  ë²¡í„°ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°\n",
    "        if embeddings_list:\n",
    "            movie_embeddings = np.vstack(embeddings_list)\n",
    "            all_embeddings.append(movie_embeddings)\n",
    "\n",
    "            # ğŸ”¹ í•´ë‹¹ ì˜í™”ì˜ IDë¥¼ ì—¬ëŸ¬ ê°œ ì¶”ê°€ (ë²¡í„° ê°œìˆ˜ë§Œí¼)\n",
    "            for _ in range(len(movie_embeddings)):\n",
    "                movie_mapping.append(idx)\n",
    "\n",
    "    # ğŸ”¹ ë²¡í„° ë°°ì—´ë¡œ ë³€í™˜ ë° ì •ê·œí™”\n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    faiss.normalize_L2(all_embeddings)\n",
    "\n",
    "    # ğŸ”¹ FAISS ì¸ë±ìŠ¤ ìƒì„±\n",
    "    d = all_embeddings.shape[1]  # ë²¡í„° ì°¨ì› ìˆ˜\n",
    "    index = faiss.IndexFlatIP(d)\n",
    "    index.add(all_embeddings)\n",
    "\n",
    "    # ğŸ”¹ ì¸ë±ìŠ¤ ì €ì¥\n",
    "    faiss.write_index(index, FAISS_INDEX_FILE)\n",
    "    np.save(MOVIE_INDICES_FILE, np.array(movie_mapping))\n",
    "\n",
    "    print(\"[INFO] FAISS ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ.\")\n",
    "\n",
    "\n",
    "# FAISS ì¸ë±ìŠ¤ ë¡œë“œ (ì—†ìœ¼ë©´ ìƒì„±)\n",
    "def load_faiss_index():\n",
    "    if os.path.exists(FAISS_INDEX_FILE) and os.path.exists(MOVIE_INDICES_FILE):\n",
    "        print(\"[INFO] ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...\")\n",
    "        index = faiss.read_index(FAISS_INDEX_FILE)\n",
    "        movie_indices = np.load(MOVIE_INDICES_FILE)\n",
    "        return index, movie_indices\n",
    "    else:\n",
    "        print(\"âŒ FAISS ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤...\")\n",
    "        build_faiss_index()  # FAISS ì¸ë±ìŠ¤ ìƒì„±\n",
    "        index = faiss.read_index(FAISS_INDEX_FILE)\n",
    "        movie_indices = np.load(MOVIE_INDICES_FILE)\n",
    "        return index, movie_indices\n",
    "    \n",
    "\n",
    "# STEP1 : ì‚¬ìš©ì ì§ˆë¬¸ LLM ë¶„ì„ ì§„í–‰.\n",
    "def analyze_question_with_llm(question):\n",
    "    prompt = f\"\"\"\n",
    "    ì‚¬ìš©ìê°€ ì˜í™” ì¶”ì²œì„ ìš”ì²­í–ˆìŠµë‹ˆë‹¤.\n",
    "    ë‹¤ìŒ ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì˜í™”ì˜ ì¥ë¥´, ë¶„ìœ„ê¸°, í‚¤ì›Œë“œ, ê°ë…, ì£¼ì—° ë“±ì˜ ì •ë³´ë¥¼ ë¶„ì„í•˜ì„¸ìš”.\n",
    "    ì§ˆë¬¸: \"{question}\"\n",
    "\n",
    "    ì´ ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¥ ì ì ˆí•œ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ì„¸ìš”.\n",
    "\n",
    "    \"í•µì‹¬ í‚¤ì›Œë“œ\"ëŠ” ì§ˆë¬¸ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ìš”ì†Œì´ë©°, íŠ¹ì •í•œ ë°°ê²½, í…Œë§ˆ, ê³„ì ˆ, ê°ì„± ë“±ì„ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "    \"í•µì‹¬ í‚¤ì›Œë“œ\"ê°€ ì—†ëŠ” ê²½ìš°ì—ëŠ” ë¹ˆ ê°’ìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”.\n",
    "\n",
    "    - ì˜ˆì œ:\n",
    "        - \"í¬ë¦¬ìŠ¤ë§ˆìŠ¤ ì˜í™” ì¶”ì²œí•´ì¤˜\" â†’ í•µì‹¬ í‚¤ì›Œë“œ: [\"í¬ë¦¬ìŠ¤ë§ˆìŠ¤\"]\n",
    "        - \"ë¹„ ì˜¤ëŠ” ë‚  ë³¼ ì˜í™” ì¶”ì²œí•´ì¤˜\" â†’ í•µì‹¬ í‚¤ì›Œë“œ: [\"ë¹„\"]\n",
    "        - \"ì—°ì¸ì´ë‘ ë³¼ë§Œí•œ ì˜í™” ì¶”ì²œí•´ì¤˜\" â†’ í•µì‹¬ í‚¤ì›Œë“œ: [\"ë¡œë§¨ìŠ¤\"]\n",
    "        - \"ì¬ë¯¸ìˆëŠ” ì˜í™” ì¶”ì²œí•´ì¤˜\" â†’ í•µì‹¬ í‚¤ì›Œë“œ: []\n",
    "        - \"ê°€ì¡±ê³¼ ë³¼ ë§Œí•œ ì˜í™” ì¶”ì²œí•´ì¤˜\" â†’ í•µì‹¬ í‚¤ì›Œë“œ: []\n",
    "    \n",
    "    \"í•µì‹¬ í‚¤ì›Œë“œ\"ëŠ” 1ê°œë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "    JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”.\n",
    "    ì˜ˆì‹œ: {{\"ì¥ë¥´\": [\"ë¡œë§¨ìŠ¤\", \"ì½”ë¯¸ë””\", \"ë“œë¼ë§ˆ\"], \"ë¶„ìœ„ê¸°\": [\"ê°ì„±ì ì¸\", \"ë”°ëœ»í•œ\"], \"í‚¤ì›Œë“œ\": [\"í¬ë¦¬ìŠ¤ë§ˆìŠ¤\", \"ê²¨ìš¸\", \"ëˆˆ\", \"ì—°ë§\"], \"ê°ë…\": \"ë´‰ì¤€í˜¸\", \"ì£¼ì—°\": \"ì†¡ê°•í˜¸\", \"ì œì™¸í•  í‚¤ì›Œë“œ\": [\"í­ë ¥\", \"ê°•ê°„\", \"ì”ì¸í•œ\"], \"í•µì‹¬ í‚¤ì›Œë“œ\" : [\"í¬ë¦¬ìŠ¤ë§ˆìŠ¤\"]}}\n",
    "    \n",
    "    ë¶„ì„ ê²°ê³¼ì— ëŒ€í•œ ì¥ë¥´, ë¶„ìœ„ê¸°, í‚¤ì›Œë“œ, ì œì™¸í•  í‚¤ì›Œë“œëŠ” 3ê°œì”© ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    ì§ˆë¬¸ì„ ë¶„ì„í•œ ë’¤ì— í•„ìš”ì¹˜ ì•ŠëŠ” KEYê°’ì€ ë°˜í™˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
    "    ì˜ˆì‹œ: \"ë´‰ì¤€í˜¸ ê°ë…ì˜ ì˜í™” ì¶”ì²œí•´ì¤˜\"ì™€ ê°™ì€ ì§ˆë¬¸ì˜ ê²½ìš°, \"ì¥ë¥´\", \"ë¶„ìœ„ê¸°\", \"í‚¤ì›Œë“œ\", \"ì£¼ì—°\", \"ì œì™¸í•  í‚¤ì›Œë“œ\" ë“±ì˜ ìš”ì†Œê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë°˜ë©´ì— \"ê²¨ìš¸ì— ë³¼ë§Œí•œ ì˜í™” ì¶”ì²œí•´ì¤˜\"ì™€ ê°™ì€ ì§ˆë¬¸ì˜ ê²½ìš°, \"ê°ë…\", \"ì£¼ì—°\" ë“±ì˜ ìš”ì†Œê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"ë‹¹ì‹ ì€ ì˜í™” ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "    # ğŸ”¹ ì‘ë‹µ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°\n",
    "    raw_content = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    print(\"[INFO] GPT ì‘ë‹µ:\", raw_content)  # ë””ë²„ê¹…ìš© ì¶œë ¥\n",
    "\n",
    "    # ğŸ”¹ JSON ì½”ë“œ ë¸”ë¡ì´ í¬í•¨ë˜ì–´ ìˆëŠ” ê²½ìš° ì²˜ë¦¬\n",
    "    if raw_content.startswith(\"```json\"):\n",
    "        raw_content = raw_content.strip(\"```json\").strip(\"```\")  # ```json ì œê±°\n",
    "\n",
    "    try:\n",
    "        parsed_json = json.loads(raw_content)\n",
    "        return parsed_json\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"âŒ JSON ë³€í™˜ ì‹¤íŒ¨:\", e)\n",
    "        return {}\n",
    "\n",
    "\n",
    "## ì œì™¸í•  í‚¤ì›Œë“œë¥¼ ê°€ì§€ê³  ìˆëŠ” ì˜í™” í•„í„°ë§í•˜ëŠ” í•¨ìˆ˜\n",
    "def filter_movies_by_exclusion(df, exclusion_keywords):\n",
    "    if not exclusion_keywords:\n",
    "        return df  # ì œì™¸í•  í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜\n",
    "\n",
    "    mask = df.apply(lambda row: not any(excl in row[\"í‚¤ì›Œë“œ(í•œê¸€)\"] for excl in exclusion_keywords), axis=1)\n",
    "    return df[mask]\n",
    "\n",
    "\n",
    "# STEP3 : FAISS ê²€ìƒ‰ í›„ í•„í„°ë§ ì§„í–‰ (í•µì‹¬ í‚¤ì›Œë“œ ë°˜ì˜ + ê°ë…, ì£¼ì—° í•„í„°ë§ ë³´ê°•)\n",
    "def search_movies_with_keywords(expanded_keywords, top_k=100):\n",
    "    \"\"\"\n",
    "    - ê°ë…ì´ë‚˜ ì£¼ì—° ë°°ìš°ê°€ ìˆëŠ” ê²½ìš°: ì „ì²´ DBì—ì„œ ì§ì ‘ í•„í„°ë§ (FAISS ì‚¬ìš© X)\n",
    "    - í•µì‹¬ í‚¤ì›Œë“œê°€ ìˆëŠ” ê²½ìš°: í•´ë‹¹ í‚¤ì›Œë“œë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ê²€ìƒ‰ í›„ ê¸°ì¡´ ê²€ìƒ‰ ê²°ê³¼ì™€ ë³‘í•©\n",
    "    \"\"\"\n",
    "    index, movie_indices = load_faiss_index()\n",
    "\n",
    "    if index is None or movie_indices is None:\n",
    "        return pd.DataFrame()  # ì¸ë±ìŠ¤ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë°ì´í„°í”„ë ˆì„ ë°˜í™˜\n",
    "\n",
    "    expanded_keywords.setdefault(\"ì¥ë¥´\", [])\n",
    "    expanded_keywords.setdefault(\"ë¶„ìœ„ê¸°\", [])\n",
    "    expanded_keywords.setdefault(\"í‚¤ì›Œë“œ\", [])\n",
    "    expanded_keywords.setdefault(\"í•µì‹¬ í‚¤ì›Œë“œ\", [])\n",
    "    expanded_keywords.setdefault(\"ì œì™¸í•  í‚¤ì›Œë“œ\", [])\n",
    "    expanded_keywords.setdefault(\"ê°ë…\", \"\")\n",
    "    expanded_keywords.setdefault(\"ì£¼ì—°\", \"\")\n",
    "\n",
    "    # âœ… í•­ìƒ ì¡´ì¬í•˜ëŠ” ë¹ˆ ë°ì´í„°í”„ë ˆì„ ì„ ì–¸ (ì—ëŸ¬ ë°©ì§€)\n",
    "    result_df = pd.DataFrame()\n",
    "\n",
    "    # ğŸ”¹ 1. ê°ë…ì´ ëª…í™•íˆ ì£¼ì–´ì§„ ê²½ìš° â†’ DBì—ì„œ ì§ì ‘ í•„í„°ë§\n",
    "    if expanded_keywords[\"ê°ë…\"]:\n",
    "        result_df = df[df[\"ê°ë…\"].str.contains(expanded_keywords[\"ê°ë…\"], na=False)]\n",
    "        result_df = filter_movies_by_exclusion(result_df, expanded_keywords[\"ì œì™¸í•  í‚¤ì›Œë“œ\"])\n",
    "        return result_df\n",
    "\n",
    "    # ğŸ”¹ 2. ì£¼ì—° ë°°ìš°ê°€ ëª…í™•íˆ ì£¼ì–´ì§„ ê²½ìš° â†’ DBì—ì„œ ì§ì ‘ í•„í„°ë§\n",
    "    if expanded_keywords[\"ì£¼ì—°\"]:\n",
    "        result_df = df[df[\"ì£¼ì—°\"].str.contains(expanded_keywords[\"ì£¼ì—°\"], na=False)]\n",
    "        result_df = filter_movies_by_exclusion(result_df, expanded_keywords[\"ì œì™¸í•  í‚¤ì›Œë“œ\"])\n",
    "        return result_df\n",
    "\n",
    "    # ğŸ”¹ 3. í•µì‹¬ í‚¤ì›Œë“œê°€ ìˆë‹¤ë©´, í•´ë‹¹ í‚¤ì›Œë“œë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ê²€ìƒ‰\n",
    "    keyword_filter_movies = pd.DataFrame()\n",
    "    if expanded_keywords[\"í•µì‹¬ í‚¤ì›Œë“œ\"]:\n",
    "        keyword_filter_movies = df[df[\"í‚¤ì›Œë“œ(í•œê¸€)\"].str.contains(\"|\".join(expanded_keywords[\"í•µì‹¬ í‚¤ì›Œë“œ\"]), na=False)]\n",
    "\n",
    "    core_keyword_results = pd.DataFrame()\n",
    "    if expanded_keywords[\"í•µì‹¬ í‚¤ì›Œë“œ\"]:\n",
    "        core_query_embedding = model.encode([\", \".join(expanded_keywords[\"í•µì‹¬ í‚¤ì›Œë“œ\"])], convert_to_numpy=True)\n",
    "        faiss.normalize_L2(core_query_embedding)\n",
    "        _, core_indices = index.search(core_query_embedding, top_k // 2)  # í•µì‹¬ í‚¤ì›Œë“œëŠ” ë”°ë¡œ ê²€ìƒ‰\n",
    "        core_keyword_results = df.iloc[movie_indices[core_indices[0]]].drop_duplicates(subset=[\"ì˜í™” ì œëª©\"])\n",
    "\n",
    "    # ğŸ”¹ 4. FAISS ê²€ìƒ‰ ìˆ˜í–‰ (ì¥ë¥´ + í‚¤ì›Œë“œ + ë¶„ìœ„ê¸° í¬í•¨)\n",
    "    query_text = expanded_keywords[\"ì¥ë¥´\"] + expanded_keywords[\"í‚¤ì›Œë“œ\"] + expanded_keywords[\"ë¶„ìœ„ê¸°\"]\n",
    "    weighted_query_text = query_text + (expanded_keywords[\"í•µì‹¬ í‚¤ì›Œë“œ\"] * 2)  # í•µì‹¬ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ 2ë°° ì ìš©\n",
    "    query_embedding = model.encode([\", \".join(weighted_query_text)], convert_to_numpy=True)\n",
    "\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    _, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    faiss_results = df.iloc[movie_indices[indices[0]]].drop_duplicates(subset=[\"ì˜í™” ì œëª©\"])\n",
    "    faiss_results = filter_movies_by_exclusion(faiss_results, expanded_keywords[\"ì œì™¸í•  í‚¤ì›Œë“œ\"])\n",
    "\n",
    "    print(\"[INFO] FAISS ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    # ğŸ”¹ 5. ë¶„ìœ„ê¸° í‚¤ì›Œë“œ ê¸°ë°˜ FAISS ê²€ìƒ‰ ì¶”ê°€ (ê¸°ì¡´ì˜ DB ì§ì ‘ ê²€ìƒ‰ ì œê±°)\n",
    "    mood_results = pd.DataFrame()\n",
    "    if expanded_keywords[\"ë¶„ìœ„ê¸°\"]:\n",
    "        mood_query_embedding = model.encode([\", \".join(expanded_keywords[\"ë¶„ìœ„ê¸°\"])], convert_to_numpy=True)\n",
    "        faiss.normalize_L2(mood_query_embedding)\n",
    "        _, mood_indices = index.search(mood_query_embedding, top_k)\n",
    "        mood_results = df.iloc[movie_indices[mood_indices[0]]].drop_duplicates(subset=[\"ì˜í™” ì œëª©\"])\n",
    "\n",
    "\n",
    "    # âœ… 6. ë¹ˆ ë°ì´í„°í”„ë ˆì„ ë¬¸ì œ í•´ê²° â†’ í•­ìƒ ì¡´ì¬í•˜ëŠ” result_dfì™€ ë³‘í•©\n",
    "    result_df = pd.concat([result_df, keyword_filter_movies, mood_results, core_keyword_results, faiss_results]).drop_duplicates(subset=[\"ì˜í™” ì œëª©\"])\n",
    "\n",
    "    print(f\"[INFO] ìµœì¢… ê²€ìƒ‰ëœ ì˜í™” ê°œìˆ˜: {len(result_df)}\")\n",
    "    return result_df\n",
    "\n",
    "\n",
    "\n",
    "# STEP4: LLMì„ í™œìš©í•œ ìµœì¢… ì¶”ì²œ ìƒì„±\n",
    "def generate_recommendations(question, search_results, max_results=5, batch_size=10):\n",
    "    if search_results.empty:\n",
    "        return []\n",
    "\n",
    "    movie_data = search_results[['ì˜í™” ì œëª©', 'ì¥ë¥´', 'ê°ë…', 'ì£¼ì—°', 'í‚¤ì›Œë“œ(í•œê¸€)']].to_dict(orient='records')\n",
    "    total_movies = len(movie_data)\n",
    "\n",
    "\n",
    "    # âœ… LLM í˜¸ì¶œì„ Batch ë‹¨ìœ„ë¡œ ìˆ˜í–‰í•˜ì—¬ RateLimit ë°©ì§€\n",
    "    recommended_movies = []\n",
    "    for i in range(0, total_movies, batch_size):\n",
    "        batch = movie_data[i:i+batch_size]\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        ë‹¹ì‹ ì€ ì˜í™”ë¥¼ ì¶”ì²œí•˜ëŠ” ì˜í™” ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n",
    "        ì‚¬ìš©ìê°€ ì˜í™” ì¶”ì²œì„ ìš”ì²­í–ˆìŠµë‹ˆë‹¤.\n",
    "        ì§ˆë¬¸: \"{question}\"\n",
    "        ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ì¶”ì²œ ê°œìˆ˜: {max_results}ê°œ\n",
    "        \n",
    "        ì•„ë˜ëŠ” ê²€ìƒ‰ëœ ì˜í™” ëª©ë¡ì…ë‹ˆë‹¤.\n",
    "        ì´ ì¤‘ì—ì„œ ì‚¬ìš©ìê°€ ìš”ì²­í•œ ì§ˆë¬¸ì— ê´€ë ¨ì—†ëŠ” ì˜í™”ëŠ” ì œì™¸í•˜ê³ , ê°€ì¥ ì ì ˆí•œ ì˜í™”ë¥¼ ì¶”ì²œí•˜ì„¸ìš”.\n",
    "        ë‹¨, ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ì—†ëŠ” ì˜í™”ëŠ” ì œì™¸í•˜ê³ , ê´€ë ¨ëœ ì˜í™”ê°€ {max_results}ê°œë³´ë‹¤ ì ë‹¤ë©´ ì ì€ ê°œìˆ˜ë§Œ ì¶”ì²œí•˜ì„¸ìš”.\n",
    "\n",
    "        ì‚¬ëŒë“¤ì—ê²Œ ì¸ì§€ë„ê°€ ë†’ê³  ê¾¸ì¤€íˆ íšŒìë˜ëŠ” ì˜í™”ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì¶”ì²œí•˜ì„¸ìš”.\n",
    "        ë‹¤ë§Œ, DBì— ì—†ëŠ” ì˜í™”ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "\n",
    "        ë‹µë³€ì„ ë°˜í™˜í•˜ê¸° ì „, ë‹¤ì‹œ í•œë²ˆ ì´ ì˜í™”ê°€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì–´ìš¸ë¦¬ëŠ” ì˜í™”ì¸ì§€ íŒë‹¨í•˜ì„¸ìš”.\n",
    "\n",
    "        JSON í˜•ì‹ìœ¼ë¡œ ì˜í™” ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ì„¸ìš”.\n",
    "        {json.dumps(batch, ensure_ascii=False)}\n",
    "\n",
    "        ì˜ˆì‹œ:\n",
    "        {{\"ì¶”ì²œ ì˜í™”\": [\"ì˜í™”1\", \"ì˜í™”2\", \"ì˜í™”3\"]}}\n",
    "        \"\"\"\n",
    "\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[{\"role\": \"system\", \"content\": \"ë‹¹ì‹ ì€ ì˜í™” ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}],\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            batch_recommendations = json.loads(response[\"choices\"][0][\"message\"][\"content\"]).get(\"ì¶”ì²œ ì˜í™”\", [])\n",
    "            recommended_movies.extend(batch_recommendations)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"âŒ LLMì—ì„œ ì˜ëª»ëœ ì‘ë‹µì„ ë°›ì•˜ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "        # âœ… ìµœëŒ€ ê²°ê³¼ ê°œìˆ˜ë§Œí¼ë§Œ ìœ ì§€\n",
    "        if len(recommended_movies) >= max_results:\n",
    "            break\n",
    "\n",
    "    return recommended_movies[:max_results]\n",
    "\n",
    "# ì‹¤í–‰\n",
    "if __name__ == \"__main__\":\n",
    "    question = input(\"ğŸ¬ ì˜í™” ì¶”ì²œ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: \")\n",
    "    \n",
    "    max_results = int(input(\"ğŸ”¢ ì¶”ì²œë°›ì„ ì˜í™” ê°œìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 5): \"))\n",
    "\n",
    "    print(\"\\n[INFO] ì‚¬ìš©ì ì§ˆë¬¸ ë¶„ì„ ì¤‘...\")\n",
    "    expanded_keywords = analyze_question_with_llm(question)\n",
    "    print(\"[INFO] ë¶„ì„ëœ ê²€ìƒ‰ í‚¤ì›Œë“œ:\", expanded_keywords)\n",
    "\n",
    "    print(\"\\n[INFO] ì˜í™” ê²€ìƒ‰ ì§„í–‰ ì¤‘...\")\n",
    "    search_results = search_movies_with_keywords(expanded_keywords)\n",
    "\n",
    "    print(\"\\n[INFO] ì¶”ì²œ ì˜í™” ì„ íƒ ì¤‘...\")\n",
    "    movie_list = generate_recommendations(question, search_results, max_results=max_results)\n",
    "\n",
    "    print(\"\\nğŸ¬ ìµœì¢… ì¶”ì²œ ì˜í™” ë¦¬ìŠ¤íŠ¸:\")\n",
    "    print(movie_list)  # ë¦¬ìŠ¤íŠ¸ ì¶œë ¥\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['í¬ì¹´í˜¼íƒ€ìŠ¤', 'ë®¬ë€', 'ê°œë¯¸', 'í”„ë¦°ì„¸ìŠ¤ ë‹¤ì´ì–´ë¦¬1', 'ìŠˆë ‰ 2', 'íŒì˜ ë¯¸ë¡œ-ì˜¤í•„ë¦¬ì•„ì™€ ì„¸ ê°œì˜ ì—´ì‡ ', 'ë²¼ë‘ ìœ„ì˜ í¬ë‡¨']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pjt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
